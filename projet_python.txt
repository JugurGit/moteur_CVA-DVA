### FILE: app_streamlit.py
# app_streamlit.py
from __future__ import annotations

import streamlit as st

from app_lib.style import apply_page_config, apply_css

# ---------------------------------------------------------------------
# 0) Setup UI (config + CSS)
# ---------------------------------------------------------------------
# On centralise ici le th√®me, le layout, les styles "pro", etc.
apply_page_config(title="R√©plication du moteur CVA/DVA", icon="üìä")
apply_css()

# ---------------------------------------------------------------------
# 1) Sidebar globale (comme dans app.py)
# ---------------------------------------------------------------------
# Objectif : donner un point d‚Äôentr√©e clair (contexte + mode d‚Äôusage),
# et stocker quelques toggles dans session_state pour les autres pages.
with st.sidebar:
    st.markdown("## XVA Lab")
    st.caption("CVA/DVA ‚Ä¢ Hull‚ÄìWhite 1F++ ‚Ä¢ log-OU ‚Ä¢ Shapley ‚Ä¢ Tracking")

    # Mode "tracking" : utile si tu historises des runs / snapshots dans l'app
    tracking = st.toggle("üìå Portfolio tracking mode", value=True)
    st.session_state["tracking_mode"] = tracking

    # Optionnel : un mode verbose global (pratique si tu veux afficher plus de logs)
    verbose = st.toggle("üß™ Mode verbose", value=False)
    st.session_state["verbose_mode"] = verbose

    st.divider()

# ---------------------------------------------------------------------
# 2) Page content (marketing / m√©moire) ‚Äî structur√© comme app.py
# ---------------------------------------------------------------------
st.title("üìä R√©plication du moteur CVA/DVA ‚Äî D√©mo technique")
st.caption("Sc√©narios taux & cr√©dit ‚Ä¢ Expositions EPE/ENE ‚Ä¢ CVA/DVA ‚Ä¢ Explain (Shapley) ‚Ä¢ Export & tra√ßabilit√©")

st.markdown("### üß© Contexte ‚Äî De Banque Palatine √† une d√©mo ‚Äúreproductible‚Äù")

st.info(
    """
Ce projet est une **d√©mo technique** inspir√©e des travaux r√©alis√©s chez **Banque Palatine** (√©quipe Risques / XVA).
L‚Äôobjectif est de **recr√©er une cha√Æne de calcul CVA/DVA** dans un cadre **structur√© comme en production** :

- **G√©n√©ration de sc√©narios** (taux) via **Hull‚ÄìWhite 1F++**  
- **Mod√©lisation des intensit√©s de d√©faut** via **log-OU** (contreparties **et** banque)  
- **Expositions** (**EPE / ENE**) puis calcul des **legs** et **totaux** de **CVA / DVA**  
- **Tra√ßabilit√©** : logs, snapshots, exports (CSV / JSON / PNG) pour reproduire et documenter un run
""",
    icon="üè¶",
)

st.warning(
    """
Je ne dispose pas des **donn√©es internes** ni de la **documentation** n√©cessaires
pour illustrer les traitements de mani√®re ‚Äúr√©elle‚Äù.
Le projet remplace donc ces entr√©es par des donn√©es **contr√¥l√©es / simul√©es**.
""",
    icon="‚ö†Ô∏è",
)

st.markdown("### üéØ Ce que d√©montre ce mini-projet (workflow end-to-end)")

cA, cB, cC, cD = st.columns(4)
with cA:
    st.markdown("**1) Hypoth√®ses ma√Ætris√©es**")
    st.caption("March√© simul√© ‚Ä¢ seeds ‚Ä¢ horizons ‚Ä¢ param√®tres mod√®les")
with cB:
    st.markdown("**2) Simulation & expositions**")
    st.caption("Trajectoires ‚Ä¢ cashflows ‚Ä¢ EPE/ENE ‚Ä¢ profils temporels")
with cC:
    st.markdown("**3) CVA/DVA ‚Äúreporting-ready‚Äù**")
    st.caption("Discounting ‚Ä¢ PD ‚Ä¢ agr√©gation buckets ‚Ä¢ r√©sultats exploitables")
with cD:
    st.markdown("**4) Explicabilit√© & tra√ßabilit√©**")
    st.caption("Shapley ‚Ä¢ contributions ‚Ä¢ exports ‚Ä¢ comparaisons de runs")

st.success(
    """
**En r√©sum√©** : une r√©plique ‚Äúmini moteur‚Äù qui illustre **la m√™me d√©marche que chez Banque Palatine** :
structurer un calcul XVA avec des inputs ma√Ætris√©s, des sorties tra√ßables, et une lecture claire des **sensibilit√©s**
(exposition, discounting, probabilit√©s de d√©faut).
""",
    icon="‚úÖ",
)

with st.expander("üîé Comment j‚Äôai structur√© l‚Äôapproche (logique ‚Äúproduction / audit‚Äù)", expanded=False):
    st.markdown(
        """
- **S√©paration des responsabilit√©s** : mod√®les/simulation (lib) vs **UI** (Streamlit) vs **stockage** (runs, snapshots).
- **Cha√Æne calcul claire** : sc√©narios ‚Üí expositions ‚Üí legs CVA/DVA ‚Üí agr√©gation ‚Üí export.
- **Explainability** : d√©composition **Shapley** pour relier un total CVA/DVA √† ses principaux contributeurs.
"""
    )

st.divider()

# ---------------------------------------------------------------------
# 3) Navigation (comme ton app.py FRTB / IR Lab)
# ---------------------------------------------------------------------
st.markdown(
    """
### üß≠ Navigation
Utilise les pages √† gauche :

- **Overview** : r√©sum√© + √©tat courant + KPIs (CVA, DVA, EPE, ENE)
- **Market / Models** : hypoth√®ses simul√©es (HW 1F++ / log-OU), param√®tres, seeds
- **Run / Simulation** : ex√©cution d‚Äôun run, suivi logs, sauvegarde des artefacts
- **Exposures** : profils EPE/ENE (agr√©g√© / par contrepartie)
- **CVA / DVA** : legs (DF, PD, expo) + totaux + vues par buckets
- **Analytics** : Shapley / contributions (DF, expo, PD) par bucket et/ou contrepartie
- **Export** : CSV / JSON / PNG pour reporting et historique

> Astuce : si le moteur imprime beaucoup, on capture les logs et on les affiche pour garder une trace du run.
"""
)

# ---------------------------------------------------------------------
# 4) (Optionnel) Affichage des logs du dernier run, si disponibles
# ---------------------------------------------------------------------
# Si tes pages "Run" stockent des logs dans session_state, ce bloc les rend accessibles depuis l'accueil.
if st.session_state.get("last_logs"):
    with st.expander("Afficher les logs du dernier run", expanded=False):
        st.code(st.session_state["last_logs"], language="text")


### FILE: export_projet.py
import os

ROOT = os.path.dirname(os.path.abspath(__file__))
OUTPUT = os.path.join(ROOT, "projet_python.txt")

EXCLUDE_DIRS = {".git", "venv", "__pycache__"}

with open(OUTPUT, "w", encoding="utf-8") as out:
    for dirpath, dirnames, filenames in os.walk(ROOT):
        # enlever les dossiers √† exclure
        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]

        for filename in filenames:
            if not filename.endswith(".py"):
                continue

            full_path = os.path.join(dirpath, filename)
            rel_path = os.path.relpath(full_path, ROOT)

            out.write(f"### FILE: {rel_path}\n")
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    out.write(f.read())
            except UnicodeDecodeError:
                out.write("[UNICODE ERROR: impossible de lire ce fichier]\n")

            out.write("\n\n")

print(f"Export termin√© dans {OUTPUT}")


### FILE: Home.py
# Home.py
import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, sidebar_run_selector, require_outdir
from app_lib.io import list_runs

apply_page_config(title="XVA HW1F++ ‚Äî Multi-pages", icon="üìä")
apply_css()

st.title("üìä XVA HW1F++ ‚Äî Interface multi-pages")
st.caption("Dashboard / drilldown contreparties / Shapley & compare / lancer un run / code browser")

runs = list_runs(str(data_dir()))
outdir = sidebar_run_selector(runs)

st.markdown("---")
if outdir is None:
    st.info("Va sur la page üöÄ Run_from_UI pour cr√©er un run, ou lance main.py pour g√©n√©rer ./data/run_...")
else:
    st.success(f"Run s√©lectionn√© : `{outdir.name}`")
    st.write("Tu peux naviguer avec les pages √† gauche :")
    st.markdown(
        """
- üìä **Dashboard** : CVA/DVA totaux, legs agr√©g√©s, downloads.
- üë• **Counterparties** : EPE/ENE, PD/Survival, legs par contrepartie.
- üß© **Shapley & Compare** : contributions DF/EPE/PD‚Ä¶ et comparaison Jan/Mar si pr√©sente.
- üöÄ **Run from UI** : lancer une simulation depuis Streamlit.
- üßæ **Code & Artefacts** : lecture des fichiers Python + listing du run.
"""
    )


### FILE: main.py
"""
Main runner: build a 20-counterparty portfolio, simulate, and export CSVs.

Usage:
  python -m main --N 10000 --out data/run1
"""

from __future__ import annotations
import os, argparse, datetime as dt
import numpy as np

from src.core.timegrid import TimeGrid
from src.sim.rng import RNG
from src.rates.termstructure.nelson_siegel import NelsonSiegel
from src.rates.hw1f import HW1FModel
from src.rates.zc_pricer import ZCAnalyticHW
from src.rates.df_curve import DFCurveOnGrid
from src.products.schedule import Schedule
from src.products.swap import Swap
from src.credit.entities import Counterparty, Bank
from src.sim.scenario_engine import Simulator
from src.io.outputs import export_everything, write_meta_json
from src.io.plots import (
    save_exposure_plot, save_credit_plot, save_xva_legs_plot, save_totals_legs_plot
)
import csv
from src.products.swap import roll_swap
from src.xva.shapley_explain import shapley_cva_legs, shapley_dva_legs
from src.io.outputs import write_matrix_csv



def build_portfolio(ns, zc, grid, rng) -> list[Counterparty]:
    """
    Create 20 counterparties:
      - LGD ~ U[35%, 55%]
      - spread0 ~ U[120, 250] bps
      - random payer/receiver
      - coupon = par ¬± {0, ¬±12.5, ¬±25} bps
      - notionals random in {1, 2, 3} millions
    """
    sched = Schedule(0.0, 5.0, 2)  # 5Y semi-annual
    r0 = ns.inst_forward(0.0)
    tmp = Swap(notional=1_000_000, direction="payer_fix", coupon=0.0, schedule=sched)
    Kpar = tmp.par_rate(0.0, r0, zc)

    cptys: list[Counterparty] = []
    directions = ["payer_fix", "receiver_fix"]
    coupon_bps = np.array([0.0, +0.00125, -0.00125, +0.0025, -0.0025])  # 0, ¬±12.5, ¬±25 bps

    for i in range(1, 21):
        cid = f"CPTY_{i:02d}"
        notional = int(np.random.default_rng().choice([1_000_000, 2_000_000, 3_000_000]))
        direction = np.random.default_rng().choice(directions)

        coupon = float(Kpar + np.random.default_rng().choice(coupon_bps))
        swap = Swap(notional=notional, direction=direction, coupon=coupon, schedule=sched)

        LGD = float(np.random.default_rng().uniform(0.35, 0.55))
        spread0 = float(np.random.default_rng().uniform(0.012, 0.025))  # 120‚Äì250 bps

        # credit params (log-OU) ‚Äî plausible IG/HY-ish
        kappa_l = 1.0 + float(np.random.default_rng().normal(0.0, 0.15))   # ~ N(1, 0.15)
        kappa_l = max(0.5, min(1.5, kappa_l))
        sigma_l = 0.35 + float(np.random.default_rng().normal(0.0, 0.05))  # ~ N(0.35, 0.05)
        sigma_l = max(0.20, min(0.55, sigma_l))

        cpty = Counterparty.from_spread(
            cid=cid,
            LGD=LGD,
            spread0=spread0,
            kappa_lambda=kappa_l,
            sigma_lambda=sigma_l,
            swap=swap,
        )
        cptys.append(cpty)
    return cptys


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--N", type=int, default=10000, help="Number of Monte Carlo scenarios")
    parser.add_argument("--out", type=str, default=None, help="Output directory (default: data/run_YYYYMMDD_HHMMSS)")
    parser.add_argument("--seed", type=int, default=12345, help="Global RNG seed")
    # dans main(): parser
    parser.add_argument("--plots", action="store_true", help="Save PNG plots for exposures/credit/CVA-DVA")
    args = parser.parse_args()

    # Output directory
    outdir = args.out
    if outdir is None:
        stamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
        outdir = os.path.join("data", f"run_{stamp}")
    os.makedirs(outdir, exist_ok=True)

    # RNG
    rng = RNG(seed=args.seed).gen

    # Grid & curve
    grid = TimeGrid(T=5.0, dt=1/12)  # 5Y monthly buckets
    ns = NelsonSiegel(beta0=0.02, beta1=-0.01, beta2=-0.02, tau=2.5)

    # HW++ fit
    hw = HW1FModel(kappa=0.60, sigma=0.01)
    hw.fit_theta_to_curve(ns, grid)
    zc = ZCAnalyticHW(hw, ns)

    # DF grid
    df_on_grid = DFCurveOnGrid(ts=ns, grid=grid)

    # Bank (LGD fixed 60%)
    bank = Bank.from_spread(LGD=0.60, spread0=0.0080, kappa_lambda=1.0, sigma_lambda=0.35)

    # Portfolio (20 cptys)
    cptys = build_portfolio(ns, zc, grid, rng)

    # Simulator
    sim = Simulator(grid=grid, rate_model=hw, zc_pricer=zc, bank=bank, rng=rng, df_curve_on_grid=df_on_grid)

    # Run
    portfolio_out = sim.run_portfolio(cptys, N=args.N)

    # Export
    export_everything(outdir, portfolio_out)

        # ==========================
    # 2nd snapshot: "Mar" as-of
    # ==========================
    t_star = 0.25  # 3 months
    k_star = grid.index_of_time(t_star)

    # DF(0, t*) from Jan snapshot, used to bring Mar values back to PV Jan
    DF_jan = df_on_grid.values()
    df_0_tstar = float(DF_jan[k_star])

    # --- (optional) define Mar market snapshot (here: same params; replace by Mar params if needed)
    ns_mar = NelsonSiegel(beta0=ns.beta0, beta1=ns.beta1, beta2=ns.beta2, tau=ns.tau)
    hw_mar = HW1FModel(kappa=hw.kappa, sigma=hw.sigma)
    hw_mar.fit_theta_to_curve(ns_mar, grid)
    zc_mar = ZCAnalyticHW(hw_mar, ns_mar)
    df_on_grid_mar = DFCurveOnGrid(ts=ns_mar, grid=grid)

    # Bank as-of Mar (here same; replace spread0 etc by Mar values)
    bank_mar = Bank.from_spread(LGD=bank.LGD, spread0=bank.spread0, kappa_lambda=bank.kappa_lambda, sigma_lambda=bank.sigma_lambda)

    # Roll the SAME swaps to as-of t_star, keep cpty identity/credit params (spread can be updated here too)
    cptys_mar = []
    for c in cptys:
        swap_rolled = roll_swap(c.swap, t_star)
        c_mar = Counterparty.from_spread(
            cid=c.cid,
            LGD=c.LGD,
            spread0=c.spread0,              # <-- change to Mar spread if you want credit move
            kappa_lambda=c.kappa_lambda,
            sigma_lambda=c.sigma_lambda,
            swap=swap_rolled,
        )
        cptys_mar.append(c_mar)

    # New simulator (fresh caches!)
    sim_mar = Simulator(
        grid=grid,
        rate_model=hw_mar,
        zc_pricer=zc_mar,
        bank=bank_mar,
        rng=np.random.default_rng(args.seed + 1),   # separate RNG stream for clarity
        df_curve_on_grid=df_on_grid_mar,
    )

    outdir_mar = os.path.join(os.path.dirname(outdir), os.path.basename(outdir) + "_MAR")
    os.makedirs(outdir_mar, exist_ok=True)

    portfolio_out_mar = sim_mar.run_portfolio(cptys_mar, N=args.N)
    export_everything(outdir_mar, portfolio_out_mar)

    # ==========================
    # Compare: PV Jan deltas
    # ==========================
    cva_jan = float(portfolio_out["totals"]["CVA_total"])
    dva_jan = float(portfolio_out["totals"]["DVA_total"])
    cva_mar = float(portfolio_out_mar["totals"]["CVA_total"])
    dva_mar = float(portfolio_out_mar["totals"]["DVA_total"])

    delta_cva_pvjan = df_0_tstar * cva_mar - cva_jan
    delta_dva_pvjan = df_0_tstar * dva_mar - dva_jan

    print("---- XVA snapshot compare (PV Jan) ----")
    print(f"DF_Jan(0,t*) = {df_0_tstar:.12g}   (t*={t_star}y, k*={k_star})")
    print(f"CVA_Jan      = {cva_jan:.12g}")
    print(f"CVA_Mar      = {cva_mar:.12g}   (as-of Mar)")
    print(f"ŒîCVA (PVJan) = {delta_cva_pvjan:.12g}")
    print(f"DVA_Jan      = {dva_jan:.12g}")
    print(f"DVA_Mar      = {dva_mar:.12g}   (as-of Mar)")
    print(f"ŒîDVA (PVJan) = {delta_dva_pvjan:.12g}")

    # Write a tiny CSV summary
    summary_path = os.path.join(outdir, "xva_compare_pvjan.csv")
    with open(summary_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["metric", "value"])
        w.writerow(["t_star_years", f"{t_star:.12g}"])
        w.writerow(["DF_Jan_0_tstar", f"{df_0_tstar:.12g}"])
        w.writerow(["CVA_Jan", f"{cva_jan:.12g}"])
        w.writerow(["CVA_Mar", f"{cva_mar:.12g}"])
        w.writerow(["Delta_CVA_PVJan", f"{delta_cva_pvjan:.12g}"])
        w.writerow(["DVA_Jan", f"{dva_jan:.12g}"])
        w.writerow(["DVA_Mar", f"{dva_mar:.12g}"])
        w.writerow(["Delta_DVA_PVJan", f"{delta_dva_pvjan:.12g}"])
    print(f"‚úÖ Compare CSV written: {summary_path}")

    # ==========================
    # Shapley explain (per cpty)
    # ==========================
    shap_dir = os.path.join(outdir, "shapley_per_counterparty")
    os.makedirs(shap_dir, exist_ok=True)

    jan_by_cid = {r["cid"]: r for r in portfolio_out["per_counterparty"]}
    mar_by_cid = {r["cid"]: r for r in portfolio_out_mar["per_counterparty"]}

    for cid, r0 in jan_by_cid.items():
        r1 = mar_by_cid[cid]

        # --- CVA: (DF, PD_cpty, EPE)
        cva_contrib, cva_delta = shapley_cva_legs(
            DF0=r0["DF"], EPE0=r0["EPE"], PD0=r0["PD_cpty"],
            DF1=r1["DF"], EPE1=r1["EPE"], PD1=r1["PD_cpty"],
            LGD_cpty=r0["LGD_cpty"],
        )
        cva_check = (cva_contrib["DF"] + cva_contrib["EPE"] + cva_contrib["PD_cpty"]) - cva_delta

        rows_cva = []
        Kp1 = len(cva_delta)
        for k in range(Kp1):
            rows_cva.append([
                k,
                f"{cva_contrib['DF'][k]:.12g}",
                f"{cva_contrib['EPE'][k]:.12g}",
                f"{cva_contrib['PD_cpty'][k]:.12g}",
                f"{cva_delta[k]:.12g}",
                f"{cva_check[k]:.12g}",
            ])
        rows_cva.append([
            "TOTAL",
            f"{cva_contrib['DF'].sum():.12g}",
            f"{cva_contrib['EPE'].sum():.12g}",
            f"{cva_contrib['PD_cpty'].sum():.12g}",
            f"{cva_delta.sum():.12g}",
            f"{cva_check.sum():.12g}",
        ])
        write_matrix_csv(
            os.path.join(shap_dir, f"shapley_cva_{cid}.csv"),
            headers=["k", "phi_DF", "phi_EPE", "phi_PD_cpty", "delta_leg", "check(phi_sum-delta)"],
            rows=rows_cva,
        )

        # --- DVA: (DF, ENE, PD_bank, S_cpty)
        dva_contrib, dva_delta = shapley_dva_legs(
            DF0=r0["DF"], ENE0=r0["ENE"], PD_bank0=r0["PD_bank"], S_cpty0=r0["S_cpty"],
            DF1=r1["DF"], ENE1=r1["ENE"], PD_bank1=r1["PD_bank"], S_cpty1=r1["S_cpty"],
            LGD_bank=r0["LGD_bank"],
        )
        dva_check = (dva_contrib["DF"] + dva_contrib["ENE"] + dva_contrib["PD_bank"] + dva_contrib["S_cpty"]) - dva_delta

        rows_dva = []
        for k in range(Kp1):
            rows_dva.append([
                k,
                f"{dva_contrib['DF'][k]:.12g}",
                f"{dva_contrib['ENE'][k]:.12g}",
                f"{dva_contrib['PD_bank'][k]:.12g}",
                f"{dva_contrib['S_cpty'][k]:.12g}",
                f"{dva_delta[k]:.12g}",
                f"{dva_check[k]:.12g}",
            ])
        rows_dva.append([
            "TOTAL",
            f"{dva_contrib['DF'].sum():.12g}",
            f"{dva_contrib['ENE'].sum():.12g}",
            f"{dva_contrib['PD_bank'].sum():.12g}",
            f"{dva_contrib['S_cpty'].sum():.12g}",
            f"{dva_delta.sum():.12g}",
            f"{dva_check.sum():.12g}",
        ])
        write_matrix_csv(
            os.path.join(shap_dir, f"shapley_dva_{cid}.csv"),
            headers=["k", "phi_DF", "phi_ENE", "phi_PD_bank", "phi_S_cpty", "delta_leg", "check(phi_sum-delta)"],
            rows=rows_dva,
        )

    print(f"‚úÖ Shapley per-counterparty CSVs written under: {shap_dir}")

    # apr√®s export_everything(outdir, portfolio_out)
    if args.plots:
        times = grid.times
        figs_dir = os.path.join(outdir, "figs")
        os.makedirs(figs_dir, exist_ok=True)

        # Totaux (legs agr√©g√©s)
        totals = portfolio_out["totals"]
        save_totals_legs_plot(
            times=times,
            CVA_legs_sum=totals["CVA_legs_sum"],
            DVA_legs_sum=totals["DVA_legs_sum"],
            outpath=os.path.join(figs_dir, "portfolio_legs.png"),
            title="Portfolio CVA/DVA legs (sum)",
        )

        # Par contrepartie
        for res in portfolio_out["per_counterparty"]:
            cid = res["cid"]
            cdir = os.path.join(figs_dir, cid)
            os.makedirs(cdir, exist_ok=True)

            save_exposure_plot(
                times=times, EPE=res["EPE"], ENE=res["ENE"],
                outpath=os.path.join(cdir, f"{cid}_exposure.png"),
                title=f"{cid} ‚Äî EPE/ENE"
            )
            save_credit_plot(
                times=times, PD=res["PD_cpty"], S=res["S_cpty"],
                outpath=os.path.join(cdir, f"{cid}_credit.png"),
                title=f"{cid} ‚Äî Credit (PD buckets & Survival)"
            )
            save_xva_legs_plot(
                times=times, CVA_leg=res["CVA_leg"], DVA_leg=res["DVA_leg"],
                outpath=os.path.join(cdir, f"{cid}_xva_legs.png"),
                title=f"{cid} ‚Äî CVA/DVA legs"
            )


    # Meta
    meta = {
        "N": args.N,
        "seed": args.seed,
        "grid": {"T": grid.T, "dt": grid.dt, "Kp1": grid.K + 1},
        "NS_params": {"beta0": ns.beta0, "beta1": ns.beta1, "beta2": ns.beta2, "tau": ns.tau},
        "HW1F": {"kappa": hw.kappa, "sigma": hw.sigma},
        "bank": {"LGD": bank.LGD, "spread0": bank.spread0, "kappa_lambda": bank.kappa_lambda, "sigma_lambda": bank.sigma_lambda},
        "notes": "No CSA, no netting, no dependence/WWR. Float leg approx 1 - P(t, T_last).",
    }
    write_meta_json(os.path.join(outdir, "run_meta.json"), meta)

    print(f"‚úÖ Done. Files written under: {outdir}")


if __name__ == "__main__":
    main()


### FILE: app_lib\io.py
# app_lib/io.py
from __future__ import annotations
import json
import pathlib
import re
from typing import Any, Optional, Dict, List, Tuple

import numpy as np
import pandas as pd
import streamlit as st

@st.cache_data(show_spinner=False)
def list_runs(data_dir: str) -> List[str]:
    p = pathlib.Path(data_dir)
    if not p.exists():
        return []
    runs = []
    for d in p.iterdir():
        if d.is_dir() and (d.name.startswith("run_") or d.name.endswith("_MAR")):
            runs.append(d.name)
    runs.sort(reverse=True)
    return runs

def _safe_read_json(path: pathlib.Path) -> Optional[dict]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None

@st.cache_data(show_spinner=False)
def read_csv(path: str) -> pd.DataFrame:
    return pd.read_csv(path)

def guess_times_from_meta(outdir: pathlib.Path, Kp1: int) -> np.ndarray:
    rm = outdir / "run_meta.json"
    if rm.exists():
        meta = _safe_read_json(rm) or {}
        grid = meta.get("grid", {})
        T = float(grid.get("T", 5.0))
        dt = float(grid.get("dt", T / max(1, (Kp1 - 1))))
        return np.arange(Kp1, dtype=float) * dt
    if Kp1 <= 1:
        return np.array([0.0])
    return np.linspace(0.0, 5.0, Kp1)

def find_counterparties_in_run(outdir: pathlib.Path) -> List[str]:
    p = outdir / "per_counterparty"
    if not p.exists():
        return []
    cids = []
    for f in p.glob("exposures_*.csv"):
        m = re.match(r"exposures_(.+)\.csv", f.name)
        if m:
            cids.append(m.group(1))
    cids.sort()
    return cids

def load_totals(outdir: pathlib.Path) -> Dict[str, Any]:
    totals_dir = outdir / "totals"
    res: Dict[str, Any] = {}

    # totals.csv
    f_tot = totals_dir / "totals.csv"
    if f_tot.exists():
        df = read_csv(str(f_tot))
        for _, row in df.iterrows():
            res[str(row["metric"])] = float(row["value"])

    # legs + DF
    for name, key in [
        ("cva_legs_sum.csv", "CVA_legs_sum"),
        ("dva_legs_sum.csv", "DVA_legs_sum"),
        ("df_curve.csv", "DF"),
    ]:
        f = totals_dir / name
        if f.exists():
            df = read_csv(str(f))
            res[key] = df.iloc[:, 1].astype(float).to_numpy()

    return res

def load_cpty_tables(outdir: pathlib.Path, cid: str) -> Dict[str, pd.DataFrame]:
    p = outdir / "per_counterparty"
    out: Dict[str, pd.DataFrame] = {}

    for base in ["exposures", "credit", "xva"]:
        f = p / f"{base}_{cid}.csv"
        if f.exists():
            out[base] = read_csv(str(f))

    meta_f = p / f"meta_{cid}.json"
    out["meta_json"] = pd.DataFrame([_safe_read_json(meta_f) or {}])

    return out

def load_shapley(outdir: pathlib.Path, cid: str) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
    p = outdir / "shapley_per_counterparty"
    if not p.exists():
        return None, None
    f_cva = p / f"shapley_cva_{cid}.csv"
    f_dva = p / f"shapley_dva_{cid}.csv"
    df_cva = read_csv(str(f_cva)) if f_cva.exists() else None
    df_dva = read_csv(str(f_dva)) if f_dva.exists() else None
    return df_cva, df_dva

def load_compare(outdir: pathlib.Path) -> Optional[pd.DataFrame]:
    f = outdir / "xva_compare_pvjan.csv"
    if f.exists():
        return read_csv(str(f))
    return None

# --- ADD at end of app_lib/io.py ---------------------------------------------
import datetime as dt

def _parse_run_timestamp(run_name: str) -> dt.datetime | None:
    # run_YYYYMMDD_HHMMSS
    try:
        if run_name.startswith("run_") and len(run_name) >= 19:
            stamp = run_name.split("_", 1)[1]
            # stamp = YYYYMMDD_HHMMSS or sometimes with suffix; take first 15 chars
            stamp = stamp[:15]
            return dt.datetime.strptime(stamp, "%Y%m%d_%H%M%S")
    except Exception:
        return None
    return None

@st.cache_data(show_spinner=False)
def load_portfolio_table(outdir: str) -> pd.DataFrame:
    """
    Build a per-counterparty table from per_counterparty/meta_*.json (fast).
    Columns: cid, CVA, DVA, LGD_cpty, LGD_bank (+ optional).
    """
    outdir_p = pathlib.Path(outdir)
    p = outdir_p / "per_counterparty"
    if not p.exists():
        return pd.DataFrame()

    rows = []
    for f in sorted(p.glob("meta_*.json")):
        d = _safe_read_json(f) or {}
        if not d:
            continue
        rows.append({
            "cid": d.get("cid", f.stem.replace("meta_", "")),
            "CVA": float(d.get("CVA", 0.0)),
            "DVA": float(d.get("DVA", 0.0)),
            "LGD_cpty": float(d.get("LGD_cpty", float("nan"))),
            "LGD_bank": float(d.get("LGD_bank", float("nan"))),
        })

    df = pd.DataFrame(rows)
    if df.empty:
        return df

    df["Net_DVA_minus_CVA"] = df["DVA"] - df["CVA"]
    df["Gross_CVA_plus_DVA"] = df["DVA"] + df["CVA"]
    df["Abs_Net"] = (df["Net_DVA_minus_CVA"]).abs()
    return df.sort_values("CVA", ascending=False).reset_index(drop=True)

@st.cache_data(show_spinner=False)
def load_totals_timeseries(data_dir: str) -> pd.DataFrame:
    """
    For all runs in ./data, read totals/totals.csv and produce time series.
    """
    data_p = pathlib.Path(data_dir)
    if not data_p.exists():
        return pd.DataFrame()

    runs = []
    for d in data_p.iterdir():
        if not d.is_dir():
            continue
        if not (d.name.startswith("run_") or d.name.endswith("_MAR")):
            continue
        ts = _parse_run_timestamp(d.name.replace("_MAR", ""))
        totals_dir = d / "totals" / "totals.csv"
        if totals_dir.exists():
            try:
                tdf = pd.read_csv(totals_dir)
                m = {str(r["metric"]): float(r["value"]) for _, r in tdf.iterrows()}
                runs.append({
                    "run": d.name,
                    "timestamp": ts,
                    "CVA_total": m.get("CVA_total", float("nan")),
                    "DVA_total": m.get("DVA_total", float("nan")),
                })
            except Exception:
                pass

    df = pd.DataFrame(runs)
    if df.empty:
        return df

    # If timestamp missing, keep lexical
    if "timestamp" in df.columns:
        df = df.sort_values(["timestamp", "run"], ascending=[True, True])
    else:
        df = df.sort_values("run")

    df["Net_DVA_minus_CVA"] = df["DVA_total"] - df["CVA_total"]
    return df.reset_index(drop=True)

@st.cache_data(show_spinner=False)
def load_cpty_metric_across_runs(data_dir: str, cid: str) -> pd.DataFrame:
    """
    Track one counterparty across runs (reads meta_{cid}.json in each run).
    """
    data_p = pathlib.Path(data_dir)
    rows = []
    for d in data_p.iterdir():
        if not d.is_dir():
            continue
        if not (d.name.startswith("run_") or d.name.endswith("_MAR")):
            continue
        ts = _parse_run_timestamp(d.name.replace("_MAR", ""))
        meta_f = d / "per_counterparty" / f"meta_{cid}.json"
        if meta_f.exists():
            dd = _safe_read_json(meta_f) or {}
            rows.append({
                "run": d.name,
                "timestamp": ts,
                "CVA": float(dd.get("CVA", float("nan"))),
                "DVA": float(dd.get("DVA", float("nan"))),
            })

    df = pd.DataFrame(rows)
    if df.empty:
        return df
    df["Net_DVA_minus_CVA"] = df["DVA"] - df["CVA"]
    if "timestamp" in df.columns:
        df = df.sort_values(["timestamp", "run"], ascending=[True, True])
    else:
        df = df.sort_values("run")
    return df.reset_index(drop=True)


### FILE: app_lib\runner.py
# app_lib/runner.py
from __future__ import annotations
import pathlib
import time
from typing import List

import numpy as np

from src.core.timegrid import TimeGrid
from src.sim.rng import RNG
from src.rates.termstructure.nelson_siegel import NelsonSiegel
from src.rates.hw1f import HW1FModel
from src.rates.zc_pricer import ZCAnalyticHW
from src.rates.df_curve import DFCurveOnGrid
from src.products.schedule import Schedule
from src.products.swap import Swap, roll_swap
from src.credit.entities import Counterparty, Bank
from src.sim.scenario_engine import Simulator
from src.io.outputs import export_everything, write_meta_json, write_matrix_csv
from src.xva.shapley_explain import shapley_cva_legs, shapley_dva_legs
from src.io.plots import (
    save_exposure_plot, save_credit_plot, save_xva_legs_plot, save_totals_legs_plot
)

def build_portfolio_deterministic(
    ns: NelsonSiegel,
    zc: ZCAnalyticHW,
    grid: TimeGrid,
    rng: np.random.Generator,
    n_counterparties: int = 20,   # --- NEW
) -> List[Counterparty]:
    if n_counterparties <= 0:
        raise ValueError("n_counterparties must be >= 1")

    sched = Schedule(0.0, 5.0, 2)  # 5Y semi-annual
    r0 = ns.inst_forward(0.0)
    tmp = Swap(notional=1_000_000, direction="payer_fix", coupon=0.0, schedule=sched)
    Kpar = tmp.par_rate(0.0, r0, zc)

    directions = ["payer_fix", "receiver_fix"]
    coupon_bps = np.array([0.0, +0.00125, -0.00125, +0.0025, -0.0025])

    width = max(2, len(str(n_counterparties)))  # CPTY_01, ... et au-del√† si >99

    cptys: List[Counterparty] = []
    for i in range(1, n_counterparties + 1):
        cid = f"CPTY_{i:0{width}d}"
        notional = int(rng.choice([1_000_000, 2_000_000, 3_000_000]))
        direction = str(rng.choice(directions))
        coupon = float(Kpar + rng.choice(coupon_bps))
        swap = Swap(notional=notional, direction=direction, coupon=coupon, schedule=sched)

        LGD = float(rng.uniform(0.35, 0.55))
        spread0 = float(rng.uniform(0.012, 0.025))

        kappa_l = float(1.0 + rng.normal(0.0, 0.15))
        kappa_l = max(0.5, min(1.5, kappa_l))
        sigma_l = float(0.35 + rng.normal(0.0, 0.05))
        sigma_l = max(0.20, min(0.55, sigma_l))

        cpty = Counterparty.from_spread(
            cid=cid, LGD=LGD, spread0=spread0,
            kappa_lambda=kappa_l, sigma_lambda=sigma_l,
            swap=swap
        )
        cptys.append(cpty)
    return cptys

def run_pipeline_and_export(
    N: int,
    seed: int,
    outdir: pathlib.Path,
    make_plots: bool,
    include_mar_and_shapley: bool,

    # --- NEW
    n_counterparties: int = 20,
) -> pathlib.Path:
    outdir.mkdir(parents=True, exist_ok=True)

    rng = RNG(seed=seed).gen

    grid = TimeGrid(T=5.0, dt=1/12)
    ns = NelsonSiegel(beta0=0.02, beta1=-0.01, beta2=-0.02, tau=2.5)
    hw = HW1FModel(kappa=0.60, sigma=0.01)
    hw.fit_theta_to_curve(ns, grid)
    zc = ZCAnalyticHW(hw, ns)

    df_on_grid = DFCurveOnGrid(ts=ns, grid=grid)
    bank = Bank.from_spread(LGD=0.60, spread0=0.0080, kappa_lambda=1.0, sigma_lambda=0.35)

    # --- NEW: portfolio param√©trable
    cptys = build_portfolio_deterministic(ns, zc, grid, rng, n_counterparties=n_counterparties)

    sim = Simulator(grid=grid, rate_model=hw, zc_pricer=zc, bank=bank, rng=rng, df_curve_on_grid=df_on_grid)
    portfolio_out = sim.run_portfolio(cptys, N=N)

    export_everything(str(outdir), portfolio_out)

    if make_plots:
        times = grid.times
        figs_dir = outdir / "figs"
        figs_dir.mkdir(exist_ok=True)

        totals = portfolio_out["totals"]
        save_totals_legs_plot(times, totals["CVA_legs_sum"], totals["DVA_legs_sum"], str(figs_dir / "portfolio_legs.png"))

        for res in portfolio_out["per_counterparty"]:
            cid = res["cid"]
            cdir = figs_dir / cid
            cdir.mkdir(exist_ok=True)

            save_exposure_plot(times, res["EPE"], res["ENE"], str(cdir / f"{cid}_exposure.png"))
            save_credit_plot(times, res["PD_cpty"], res["S_cpty"], str(cdir / f"{cid}_credit.png"))
            save_xva_legs_plot(times, res["CVA_leg"], res["DVA_leg"], str(cdir / f"{cid}_xva_legs.png"))

    # --- NEW: stocker n_counterparties dans run_meta.json
    meta = {
        "N": N,
        "seed": seed,
        "n_counterparties": int(n_counterparties),
        "grid": {"T": grid.T, "dt": grid.dt, "Kp1": grid.K + 1},
        "NS_params": {"beta0": ns.beta0, "beta1": ns.beta1, "beta2": ns.beta2, "tau": ns.tau},
        "HW1F": {"kappa": hw.kappa, "sigma": hw.sigma},
        "bank": {"LGD": bank.LGD, "spread0": bank.spread0, "kappa_lambda": bank.kappa_lambda, "sigma_lambda": bank.sigma_lambda},
        "notes": "No CSA, no netting, no dependence/WWR. Float leg approx 1 - P(t, T_last).",
    }
    write_meta_json(str(outdir / "run_meta.json"), meta)

    if include_mar_and_shapley:
        t_star = 0.25
        k_star = grid.index_of_time(t_star)
        DF_jan = df_on_grid.values()
        df_0_tstar = float(DF_jan[k_star])

        ns_mar = NelsonSiegel(beta0=ns.beta0, beta1=ns.beta1, beta2=ns.beta2, tau=ns.tau)
        hw_mar = HW1FModel(kappa=hw.kappa, sigma=hw.sigma)
        hw_mar.fit_theta_to_curve(ns_mar, grid)
        zc_mar = ZCAnalyticHW(hw_mar, ns_mar)
        df_on_grid_mar = DFCurveOnGrid(ts=ns_mar, grid=grid)

        bank_mar = Bank.from_spread(LGD=bank.LGD, spread0=bank.spread0, kappa_lambda=bank.kappa_lambda, sigma_lambda=bank.sigma_lambda)

        cptys_mar = []
        for c in cptys:
            swap_rolled = roll_swap(c.swap, t_star)
            c_mar = Counterparty.from_spread(
                cid=c.cid,
                LGD=c.LGD,
                spread0=c.spread0,
                kappa_lambda=c.kappa_lambda,
                sigma_lambda=c.sigma_lambda,
                swap=swap_rolled,
            )
            cptys_mar.append(c_mar)

        sim_mar = Simulator(
            grid=grid, rate_model=hw_mar, zc_pricer=zc_mar,
            bank=bank_mar,
            rng=np.random.default_rng(seed + 1),
            df_curve_on_grid=df_on_grid_mar,
        )

        outdir_mar = outdir.parent / f"{outdir.name}_MAR"
        outdir_mar.mkdir(exist_ok=True)
        portfolio_out_mar = sim_mar.run_portfolio(cptys_mar, N=N)
        export_everything(str(outdir_mar), portfolio_out_mar)

        cva_jan = float(portfolio_out["totals"]["CVA_total"])
        dva_jan = float(portfolio_out["totals"]["DVA_total"])
        cva_mar = float(portfolio_out_mar["totals"]["CVA_total"])
        dva_mar = float(portfolio_out_mar["totals"]["DVA_total"])

        delta_cva_pvjan = df_0_tstar * cva_mar - cva_jan
        delta_dva_pvjan = df_0_tstar * dva_mar - dva_jan

        rows = [
            ["t_star_years", f"{t_star:.12g}"],
            ["DF_Jan_0_tstar", f"{df_0_tstar:.12g}"],
            ["CVA_Jan", f"{cva_jan:.12g}"],
            ["CVA_Mar", f"{cva_mar:.12g}"],
            ["Delta_CVA_PVJan", f"{delta_cva_pvjan:.12g}"],
            ["DVA_Jan", f"{dva_jan:.12g}"],
            ["DVA_Mar", f"{dva_mar:.12g}"],
            ["Delta_DVA_PVJan", f"{delta_dva_pvjan:.12g}"],
        ]
        write_matrix_csv(str(outdir / "xva_compare_pvjan.csv"), headers=["metric", "value"], rows=rows)

        shap_dir = outdir / "shapley_per_counterparty"
        shap_dir.mkdir(exist_ok=True)

        jan_by_cid = {r["cid"]: r for r in portfolio_out["per_counterparty"]}
        mar_by_cid = {r["cid"]: r for r in portfolio_out_mar["per_counterparty"]}

        for cid, r0 in jan_by_cid.items():
            r1 = mar_by_cid[cid]

            cva_contrib, cva_delta = shapley_cva_legs(
                DF0=r0["DF"], EPE0=r0["EPE"], PD0=r0["PD_cpty"],
                DF1=r1["DF"], EPE1=r1["EPE"], PD1=r1["PD_cpty"],
                LGD_cpty=r0["LGD_cpty"],
            )
            cva_check = (cva_contrib["DF"] + cva_contrib["EPE"] + cva_contrib["PD_cpty"]) - cva_delta

            rows_cva = []
            Kp1 = len(cva_delta)
            for k in range(Kp1):
                rows_cva.append([
                    k,
                    f"{cva_contrib['DF'][k]:.12g}",
                    f"{cva_contrib['EPE'][k]:.12g}",
                    f"{cva_contrib['PD_cpty'][k]:.12g}",
                    f"{cva_delta[k]:.12g}",
                    f"{cva_check[k]:.12g}",
                ])
            rows_cva.append([
                "TOTAL",
                f"{cva_contrib['DF'].sum():.12g}",
                f"{cva_contrib['EPE'].sum():.12g}",
                f"{cva_contrib['PD_cpty'].sum():.12g}",
                f"{cva_delta.sum():.12g}",
                f"{cva_check.sum():.12g}",
            ])
            write_matrix_csv(
                str(shap_dir / f"shapley_cva_{cid}.csv"),
                headers=["k", "phi_DF", "phi_EPE", "phi_PD_cpty", "delta_leg", "check(phi_sum-delta)"],
                rows=rows_cva,
            )

            dva_contrib, dva_delta = shapley_dva_legs(
                DF0=r0["DF"], ENE0=r0["ENE"], PD_bank0=r0["PD_bank"], S_cpty0=r0["S_cpty"],
                DF1=r1["DF"], ENE1=r1["ENE"], PD_bank1=r1["PD_bank"], S_cpty1=r1["S_cpty"],
                LGD_bank=r0["LGD_bank"],
            )
            dva_check = (dva_contrib["DF"] + dva_contrib["ENE"] + dva_contrib["PD_bank"] + dva_contrib["S_cpty"]) - dva_delta

            rows_dva = []
            for k in range(Kp1):
                rows_dva.append([
                    k,
                    f"{dva_contrib['DF'][k]:.12g}",
                    f"{dva_contrib['ENE'][k]:.12g}",
                    f"{dva_contrib['PD_bank'][k]:.12g}",
                    f"{dva_contrib['S_cpty'][k]:.12g}",
                    f"{dva_delta[k]:.12g}",
                    f"{dva_check[k]:.12g}",
                ])
            rows_dva.append([
                "TOTAL",
                f"{dva_contrib['DF'].sum():.12g}",
                f"{dva_contrib['ENE'].sum():.12g}",
                f"{dva_contrib['PD_bank'].sum():.12g}",
                f"{dva_contrib['S_cpty'].sum():.12g}",
                f"{dva_delta.sum():.12g}",
                f"{dva_check.sum():.12g}",
            ])
            write_matrix_csv(
                str(shap_dir / f"shapley_dva_{cid}.csv"),
                headers=["k", "phi_DF", "phi_ENE", "phi_PD_bank", "phi_S_cpty", "delta_leg", "check(phi_sum-delta)"],
                rows=rows_dva,
            )

    return outdir

def default_run_name() -> str:
    return time.strftime("run_%Y%m%d_%H%M%S")


### FILE: app_lib\state.py
# app_lib/state.py
from __future__ import annotations
import pathlib
import streamlit as st

def project_root() -> pathlib.Path:
    # app_lib/state.py -> app_lib -> ROOT
    return pathlib.Path(__file__).resolve().parents[1]

def data_dir() -> pathlib.Path:
    return project_root() / "data"

def get_selected_run() -> str | None:
    return st.session_state.get("selected_run", None)

def set_selected_run(run_name: str) -> None:
    st.session_state["selected_run"] = run_name

def selected_outdir() -> pathlib.Path | None:
    r = get_selected_run()
    if not r:
        return None
    return data_dir() / r

def sidebar_run_selector(runs: list[str], *, show_refresh: bool = True) -> pathlib.Path | None:
    st.sidebar.title("‚öôÔ∏è Run selection")

    if show_refresh:
        if st.sidebar.button("üîÑ Refresh runs", use_container_width=True):
            # IMPORTANT: clear cached directory scans / CSV reads
            try:
                st.cache_data.clear()
            except Exception:
                pass
            st.rerun()

    if not runs:
        st.sidebar.warning("Aucun run d√©tect√© dans ./data.")
        return None

    current = get_selected_run()
    if current not in runs:
        current = runs[0]
        set_selected_run(current)

    picked = st.sidebar.selectbox("Run", runs, index=runs.index(current))
    set_selected_run(picked)
    return selected_outdir()

def require_outdir(outdir):
    if outdir is None or (not outdir.exists()):
        st.info("S√©lectionne un run existant dans la sidebar (ou cr√©e-en un via la page üöÄ Run).")
        st.stop()


### FILE: app_lib\style.py
# app_lib/style.py
import streamlit as st

def apply_page_config(title: str = "XVA HW1F++ ‚Äî Streamlit", icon: str = "üìä"):
    st.set_page_config(page_title=title, page_icon=icon, layout="wide")

def apply_css():
    css = """
    <style>
    /* Layout */
    .block-container { padding-top: 1.1rem; padding-bottom: 2.2rem; }
    section[data-testid="stSidebar"] { width: 340px !important; }
    [data-testid="stSidebar"] > div:first-child { padding-top: 1rem; }

    /* Headings */
    h1, h2, h3 { letter-spacing: -0.02em; }
    h1 { font-weight: 800; }
    h2 { font-weight: 750; }

    /* Cards / Metrics */
    [data-testid="stMetric"] {
        border-radius: 16px;
        padding: 14px 16px;
        border: 1px solid rgba(255,255,255,0.08);
        background: rgba(255,255,255,0.03);
        box-shadow: 0 6px 16px rgba(0,0,0,0.25);
    }
    [data-testid="stMetricLabel"] p { font-size: 0.85rem; opacity: 0.9; }
    [data-testid="stMetricValue"] { font-size: 1.55rem; }

    /* Tables */
    div[data-testid="stDataFrame"]{
        border-radius: 16px;
        border: 1px solid rgba(255,255,255,0.08);
        overflow: hidden;
    }

    /* Inputs */
    .stTextInput input, .stNumberInput input, .stSelectbox div, .stMultiSelect div {
        border-radius: 12px !important;
    }

    /* Buttons */
    .stButton button {
        border-radius: 14px;
        padding: 0.55rem 0.85rem;
        border: 1px solid rgba(255,255,255,0.10);
    }

    /* Code blocks */
    pre {
        border-radius: 16px !important;
        border: 1px solid rgba(255,255,255,0.10);
        background: rgba(255,255,255,0.03) !important;
    }

    /* Subtle separators */
    hr { opacity: 0.25; }

    </style>
    """
    st.markdown(css, unsafe_allow_html=True)


### FILE: app_lib\__init__.py


### FILE: pages\1_üìä_Dashboard.py
# pages/1_üìä_Dashboard.py
import numpy as np
import pandas as pd
import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, sidebar_run_selector, require_outdir
from app_lib.io import list_runs, load_totals, guess_times_from_meta

apply_page_config(title="Dashboard ‚Äî XVA", icon="üìä")
apply_css()

runs = list_runs(str(data_dir()))
outdir = sidebar_run_selector(runs)
require_outdir(outdir)

st.title("üìä Dashboard")

totals = load_totals(outdir)
cva_total = float(totals.get("CVA_total", np.nan))
dva_total = float(totals.get("DVA_total", np.nan))
CVA_legs_sum = totals.get("CVA_legs_sum", None)
DVA_legs_sum = totals.get("DVA_legs_sum", None)
DF = totals.get("DF", None)

Kp1 = 61
if isinstance(CVA_legs_sum, np.ndarray):
    Kp1 = CVA_legs_sum.shape[0]
elif isinstance(DF, np.ndarray):
    Kp1 = DF.shape[0]
times = guess_times_from_meta(outdir, int(Kp1))

c1, c2, c3 = st.columns(3)
c1.metric("CVA total", f"{cva_total:,.6g}")
c2.metric("DVA total", f"{dva_total:,.6g}")
if isinstance(DF, np.ndarray):
    c3.metric("DF(0, 5Y)", f"{float(DF[-1]):.6g}")
else:
    c3.metric("DF(0, 5Y)", "‚Äî")

st.subheader("Legs agr√©g√©s (portfolio)")
if isinstance(CVA_legs_sum, np.ndarray) and isinstance(DVA_legs_sum, np.ndarray):
    df_plot = pd.DataFrame({
        "t": times,
        "Œ£ CVA_leg": CVA_legs_sum,
        "Œ£ DVA_leg": DVA_legs_sum,
    })
    st.line_chart(df_plot.set_index("t"))
else:
    st.warning("Impossible de charger cva_legs_sum.csv / dva_legs_sum.csv.")

png = outdir / "figs" / "portfolio_legs.png"
if png.exists():
    st.image(str(png), caption="PNG export√© (si plots activ√©s)")

st.divider()
st.subheader("Downloads rapides")

cand = [
    outdir / "meta.json",
    outdir / "run_meta.json",
    outdir / "xva_compare_pvjan.csv",
    outdir / "totals" / "totals.csv",
]
for f in cand:
    if f.exists():
        st.download_button(f"‚¨áÔ∏è {f.name}", data=f.read_bytes(), file_name=f.name, use_container_width=True)


### FILE: pages\2_üë•_Counterparties.py
# pages/2_üë•_Counterparties.py
import numpy as np
import pandas as pd
import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, sidebar_run_selector, require_outdir
from app_lib.io import list_runs, find_counterparties_in_run, load_cpty_tables, load_totals, guess_times_from_meta

apply_page_config(title="Counterparties ‚Äî XVA", icon="üë•")
apply_css()

runs = list_runs(str(data_dir()))
outdir = sidebar_run_selector(runs)
require_outdir(outdir)

st.title("üë• Contreparties")

cids = find_counterparties_in_run(outdir)
if not cids:
    st.warning("Aucune contrepartie trouv√©e (exposures_*.csv absent).")
    st.stop()

cid = st.selectbox("Choisir une contrepartie", cids, index=0)
tables = load_cpty_tables(outdir, cid)

# Try infer times length
totals = load_totals(outdir)
CVA_legs_sum = totals.get("CVA_legs_sum", None)
DF = totals.get("DF", None)
Kp1 = 61
if isinstance(CVA_legs_sum, np.ndarray):
    Kp1 = CVA_legs_sum.shape[0]
elif isinstance(DF, np.ndarray):
    Kp1 = DF.shape[0]
times = guess_times_from_meta(outdir, int(Kp1))

cA, cB, cC = st.columns([1.2, 1.2, 1.0])
with cC:
    st.write("Meta (cpty)")
    st.dataframe(tables["meta_json"], use_container_width=True)

with cA:
    st.write("Exposures (EPE/ENE)")
    if "exposures" in tables:
        df_e = tables["exposures"].copy()
        df_e["t"] = times[: len(df_e)]
        st.line_chart(df_e[["t", "EPE", "ENE"]].set_index("t"))
        st.dataframe(df_e.head(10), use_container_width=True)
    else:
        st.info("exposures_{cid}.csv absent")

with cB:
    st.write("Credit (PD / Survival)")
    if "credit" in tables:
        df_c = tables["credit"].copy()
        df_c["t"] = times[: len(df_c)]
        st.line_chart(df_c[["t", "S_cpty"]].set_index("t"))
        st.bar_chart(df_c.set_index("t")["PD_cpty"])
        st.dataframe(df_c.head(10), use_container_width=True)
    else:
        st.info("credit_{cid}.csv absent")

st.subheader("XVA legs")
if "xva" in tables:
    df_x = tables["xva"].copy()
    df_x_num = df_x[pd.to_numeric(df_x["k"], errors="coerce").notna()].copy()
    df_x_num["k"] = df_x_num["k"].astype(int)
    df_x_num["t"] = times[: len(df_x_num)]
    st.line_chart(df_x_num[["t", "CVA_leg", "DVA_leg"]].set_index("t"))
    st.dataframe(df_x.tail(8), use_container_width=True)

st.divider()
figs_dir = outdir / "figs" / cid
if figs_dir.exists():
    img1 = figs_dir / f"{cid}_exposure.png"
    img2 = figs_dir / f"{cid}_credit.png"
    img3 = figs_dir / f"{cid}_xva_legs.png"
    cols = st.columns(3)
    if img1.exists(): cols[0].image(str(img1), caption="Exposure PNG")
    if img2.exists(): cols[1].image(str(img2), caption="Credit PNG")
    if img3.exists(): cols[2].image(str(img3), caption="XVA legs PNG")


### FILE: pages\3_üß©_Shapley_Compare.py
# pages/3_üß©_Shapley_Compare.py
import numpy as np
import pandas as pd
import streamlit as st
from pathlib import Path  # <-- NEW

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, sidebar_run_selector, require_outdir
from app_lib.io import (
    list_runs, find_counterparties_in_run, load_shapley, load_compare,
    load_totals, guess_times_from_meta
)

apply_page_config(title="Shapley & Compare ‚Äî XVA", icon="üß©")
apply_css()

runs = list_runs(str(data_dir()))

# --- NEW: default run should NOT end with "MAR"
def _run_name(x) -> str:
    return Path(str(x)).name

def _is_mar_run(x) -> bool:
    return _run_name(x).strip().upper().endswith("MAR")

# stable sort: non-MAR first, MAR last
if runs:
    runs = sorted(runs, key=_is_mar_run)

outdir = sidebar_run_selector(runs)
require_outdir(outdir)

st.title("üß© Shapley & Compare (Jan/Mar)")

cids = find_counterparties_in_run(outdir)
if not cids:
    st.warning("Aucune contrepartie trouv√©e.")
    st.stop()

# times
totals = load_totals(outdir)
DF = totals.get("DF", None)
Kp1 = DF.shape[0] if isinstance(DF, np.ndarray) else 61
times = guess_times_from_meta(outdir, int(Kp1))

cid = st.selectbox("Contrepartie (Shapley)", cids, index=0)
df_cva, df_dva = load_shapley(outdir, cid)

col1, col2 = st.columns(2)
with col1:
    st.write("Shapley ‚Äî CVA legs (DF / EPE / PD_cpty)")
    if df_cva is None:
        st.info("Pas de fichier shapley_cva_*.csv trouv√© dans shapley_per_counterparty/.")
    else:
        df_num = df_cva[pd.to_numeric(df_cva["k"], errors="coerce").notna()].copy()
        df_num["k"] = df_num["k"].astype(int)
        df_num["t"] = times[: len(df_num)]
        st.area_chart(df_num[["t", "phi_DF", "phi_EPE", "phi_PD_cpty"]].set_index("t"))
        st.dataframe(df_cva.tail(10), use_container_width=True)

with col2:
    st.write("Shapley ‚Äî DVA legs (DF / ENE / PD_bank / S_cpty)")
    if df_dva is None:
        st.info("Pas de fichier shapley_dva_*.csv trouv√© dans shapley_per_counterparty/.")
    else:
        df_num = df_dva[pd.to_numeric(df_dva["k"], errors="coerce").notna()].copy()
        df_num["k"] = df_num["k"].astype(int)
        df_num["t"] = times[: len(df_num)]
        st.area_chart(df_num[["t", "phi_DF", "phi_ENE", "phi_PD_bank", "phi_S_cpty"]].set_index("t"))
        st.dataframe(df_dva.tail(10), use_container_width=True)

st.divider()
st.subheader("Compare Jan/Mar (PV Jan)")
comp = load_compare(outdir)
if comp is None:
    st.info("xva_compare_pvjan.csv absent (activer l‚Äôoption snapshot Mar + compare + shapley).")
else:
    st.dataframe(comp, use_container_width=True)


### FILE: pages\4_üöÄ_Run_from_UI.py
# pages/4_üöÄ_Run_from_UI.py
import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, set_selected_run
from app_lib.io import list_runs
from app_lib.runner import run_pipeline_and_export, default_run_name

apply_page_config(title="Run from UI ‚Äî XVA", icon="üöÄ")
apply_css()

st.title("üöÄ Lancer un nouveau run (depuis Streamlit)")

DATA = data_dir()
DATA.mkdir(exist_ok=True)

with st.sidebar:
    st.subheader("Param√®tres")

    # --- NEW: nombre de contreparties
    n_cptys = st.number_input(
        "Nombre de contreparties",
        min_value=1,
        max_value=500,
        value=20,
        step=1,
        help="Taille du portfolio simul√© (CPTY_01..). Attention: plus c'est grand, plus c'est long.",
    )

    N = st.number_input("Nombre de sc√©narios (N)", min_value=100, max_value=200000, value=5000, step=500)
    seed = st.number_input("Seed", min_value=0, max_value=2_000_000_000, value=12345, step=1)
    make_plots = st.checkbox("G√©n√©rer PNG (plots)", value=False)
    include_mar = st.checkbox("Inclure snapshot Mar + compare + Shapley", value=True)
    run_name = st.text_input("Nom du run (optionnel)", value=default_run_name())

# Helper: clear cached file scans
def _clear_run_caches():
    try:
        list_runs.clear()
    except Exception:
        pass
    try:
        st.cache_data.clear()
    except Exception:
        pass

if st.button("üöÄ Run", use_container_width=True):
    outdir = DATA / run_name.strip()

    with st.status("Simulation en cours‚Ä¶", expanded=True) as status:
        st.write("Monte-Carlo + exports‚Ä¶")
        run_pipeline_and_export(
            N=int(N),
            seed=int(seed),
            outdir=outdir,
            make_plots=make_plots,
            include_mar_and_shapley=include_mar,

            # --- NEW
            n_counterparties=int(n_cptys),
        )
        status.update(label=f"‚úÖ Run termin√©: {outdir.name}", state="complete")

    _clear_run_caches()

    set_selected_run(outdir.name)
    st.success(f"Run cr√©√© et s√©lectionn√© : {outdir.name}")
    st.rerun()

st.markdown("---")
st.subheader("Runs existants")

c1, c2 = st.columns([0.35, 0.65])
with c1:
    if st.button("üîÑ Refresh runs", use_container_width=True):
        _clear_run_caches()
        st.rerun()

runs = list_runs(str(DATA))
st.write(runs if runs else "Aucun run.")


### FILE: pages\5_üßæ_Documentation.py
# pages/5_üßæ_Code_Artefacts.py
from __future__ import annotations

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import project_root

try:
    from pages.code_docs import render_doc_panel as _render_doc_panel_manual
    _DOC_IMPORT_ERROR = None
except Exception as e:
    _render_doc_panel_manual = None
    _DOC_IMPORT_ERROR = repr(e)

apply_page_config(title="Documentation du code ‚Äî CVA-DVA", icon="üßæ")
apply_css()

ROOT = project_root()
EXCLUDE_PROJECT = {
    ".git", "venv", "__pycache__", ".pytest_cache", ".mypy_cache",
    ".streamlit_app_cache", "data"
}

# "Text-like" files we can preview without extra deps
TEXT_EXTS = {
    ".py": "python",
    ".ipynb": "ipynb",
    ".md": "markdown",
    ".txt": "text",
    ".json": "json",
    ".yaml": "yaml",
    ".yml": "yaml",
    ".toml": "toml",
    ".ini": "ini",
    ".cfg": "ini",
    ".csv": "csv",
    ".log": "text",
}

MAX_PREVIEW_BYTES = 1_200_000  # ~1.2MB

DOCS_REGISTRY_PATH = ROOT / "pages" / "docs_registry.json"


# =============================================================================
# Small utils
# =============================================================================
def _fmt_bytes(n: int) -> str:
    if n < 1024:
        return f"{n} B"
    if n < 1024 * 1024:
        return f"{n/1024:.1f} KB"
    return f"{n/(1024*1024):.2f} MB"


def _fmt_dt(ts: float) -> str:
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")


def _read_text_safely(path: Path, max_bytes: int = MAX_PREVIEW_BYTES) -> Tuple[str, bool]:
    """
    Returns (text, truncated).
    Decodes as UTF-8 with replacement; truncates by bytes if too large.
    """
    b = path.read_bytes()
    truncated = False
    if len(b) > max_bytes:
        b = b[:max_bytes]
        truncated = True
    text = b.decode("utf-8", errors="replace")
    return text, truncated


def _language_for(path: Path) -> str:
    ext = path.suffix.lower()
    if ext == ".ipynb":
        return "json"
    return TEXT_EXTS.get(ext, "text")


# =============================================================================
# Project file scanner (cached)
# =============================================================================
@st.cache_data(show_spinner=False)
def _scan_files_project(root: str) -> List[Dict[str, Any]]:
    """
    Scan project files once and cache.
    Returns list of dicts with metadata.
    """
    root_p = Path(root)
    out: List[Dict[str, Any]] = []

    for dirpath, dirnames, filenames in os.walk(root_p):
        # exclude dirs
        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_PROJECT]

        for fn in filenames:
            p = Path(dirpath) / fn
            ext = p.suffix.lower()

            if ext not in TEXT_EXTS:
                continue

            try:
                stt = p.stat()
                size = int(stt.st_size)
                mtime = float(stt.st_mtime)
            except OSError:
                continue

            rel = p.relative_to(root_p).as_posix()
            folder = str(Path(rel).parent).replace("\\", "/")
            if folder == ".":
                folder = ""

            n_lines: Optional[int] = None
            if ext != ".ipynb" and size <= 400_000:
                try:
                    txt = p.read_text(encoding="utf-8", errors="ignore")
                    n_lines = txt.count("\n") + 1 if txt else 0
                except Exception:
                    n_lines = None

            out.append(
                {
                    "rel": rel,
                    "path": str(p),
                    "ext": ext,
                    "folder": folder,
                    "size": size,
                    "mtime": mtime,
                    "lines": n_lines,
                }
            )

    out.sort(key=lambda d: d["rel"])
    return out


@st.cache_data(show_spinner=False)
def _load_docs_registry(path: str) -> Dict[str, Any]:
    p = Path(path)
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}


# =============================================================================
# Notebook renderer (simple)
# =============================================================================
def _render_notebook(path: Path) -> None:
    raw, truncated = _read_text_safely(path)
    if truncated:
        st.warning(
            f"Notebook trop volumineux : preview tronqu√©e √† {_fmt_bytes(MAX_PREVIEW_BYTES)}. "
            "Tu peux le t√©l√©charger pour l‚Äôouvrir complet.",
            icon="‚ö†Ô∏è",
        )

    try:
        nb = json.loads(raw)
    except Exception:
        st.error("Impossible de parser ce .ipynb (JSON invalide).", icon="‚ùå")
        st.code(raw, language="json")
        return

    cells = nb.get("cells", [])
    if not isinstance(cells, list):
        st.error("Format .ipynb inattendu (cells manquant).", icon="‚ùå")
        st.code(raw, language="json")
        return

    show_outputs = st.checkbox("Afficher outputs", value=False, key="ca_show_nb_outputs")
    st.divider()

    for i, cell in enumerate(cells, start=1):
        cell_type = cell.get("cell_type", "")
        src = cell.get("source", "")

        if isinstance(src, list):
            src_text = "".join(src)
        else:
            src_text = str(src)

        if cell_type == "markdown":
            if src_text.strip():
                st.markdown(src_text)
        elif cell_type == "code":
            st.markdown(f"**In [{i}]**")
            st.code(src_text, language="python")

            if show_outputs:
                outs = cell.get("outputs", [])
                if isinstance(outs, list) and outs:
                    for out in outs:
                        otype = out.get("output_type", "")
                        if otype == "stream":
                            txt = out.get("text", "")
                            if isinstance(txt, list):
                                txt = "".join(txt)
                            st.code(str(txt), language="text")
                        elif otype in ("execute_result", "display_data"):
                            data = out.get("data", {})
                            txt = None
                            if isinstance(data, dict):
                                txt = data.get("text/plain", None)
                            if txt is not None:
                                if isinstance(txt, list):
                                    txt = "".join(txt)
                                st.code(str(txt), language="text")
                        elif otype == "error":
                            tb = out.get("traceback", [])
                            if isinstance(tb, list):
                                st.code("\n".join(tb), language="text")


# =============================================================================
# UI
# =============================================================================
st.title("üßæ Documentation du code")
st.caption("Preview + documentation manuelle (docs_registry.json).")

# --- Force manual doc only
if _render_doc_panel_manual is None:
    st.error(
        "Documentation manuelle indisponible : impossible d‚Äôimporter `pages/code_docs.py`.\n\n"
        f"Erreur: `{_DOC_IMPORT_ERROR}`",
        icon="‚ùå",
    )
    st.stop()

# --- Files restricted to docs_registry.json only
all_files = _scan_files_project(str(ROOT))
docs_registry = _load_docs_registry(str(DOCS_REGISTRY_PATH))
allowed_rels = set(docs_registry.keys())

all_files = [d for d in all_files if d["rel"] in allowed_rels]
all_files.sort(key=lambda d: d["rel"])

if not docs_registry:
    st.warning(f"`docs_registry.json` introuvable ou vide: {DOCS_REGISTRY_PATH}", icon="‚ö†Ô∏è")

if not all_files:
    st.info("Aucun fichier (pr√©sent dans docs_registry.json) n‚Äôa √©t√© trouv√© dans le projet.", icon="‚ÑπÔ∏è")
    st.stop()

rels = [d["rel"] for d in all_files]

# -----------------------------
# Selection state + navigation
# -----------------------------
if "ca_px_choice" not in st.session_state:
    st.session_state["ca_px_choice"] = rels[0]
if st.session_state["ca_px_choice"] not in rels:
    st.session_state["ca_px_choice"] = rels[0]

nav1, nav2, nav3, nav4 = st.columns([0.18, 0.18, 1.0, 0.28], gap="small")
with nav1:
    if st.button("‚¨ÖÔ∏è Prev", use_container_width=True, key="ca_px_prev"):
        i = rels.index(st.session_state["ca_px_choice"])
        st.session_state["ca_px_choice"] = rels[max(0, i - 1)]
with nav2:
    if st.button("Next ‚û°Ô∏è", use_container_width=True, key="ca_px_next"):
        i = rels.index(st.session_state["ca_px_choice"])
        st.session_state["ca_px_choice"] = rels[min(len(rels) - 1, i + 1)]
with nav4:
    st.caption(f"{rels.index(st.session_state['ca_px_choice']) + 1} / {len(rels)}")

choice = st.selectbox(
    "Select file",
    rels,
    index=rels.index(st.session_state["ca_px_choice"]),
    key="ca_px_select",
)
st.session_state["ca_px_choice"] = choice

path = ROOT / choice
meta = next(d for d in all_files if d["rel"] == choice)

# -----------------------------
# Header + actions
# -----------------------------
st.write(f"**{choice}**")
st.caption(
    f"Ext: `{meta['ext']}`  |  Size: {_fmt_bytes(meta['size'])}  |  Modified: {_fmt_dt(meta['mtime'])}"
    + (f"  |  Lines: {meta['lines']}" if meta.get("lines") is not None else "")
)

try:
    file_bytes = path.read_bytes()
    st.download_button(
        "‚¨áÔ∏è Download file",
        data=file_bytes,
        file_name=path.name,
        mime="text/plain",
        use_container_width=False,
        key="ca_px_dl",
    )
except Exception:
    st.warning("Impossible de pr√©parer le t√©l√©chargement (droits/IO).", icon="‚ö†Ô∏è")

# -----------------------------
# Preview + manual doc (always)
# -----------------------------
left, right = st.columns([1.35, 0.85], gap="large")
ext = path.suffix.lower()

with left:
    st.subheader("Preview")

    if ext == ".ipynb":
        view_mode = st.radio(
            "Notebook view",
            ["Rendered", "Raw JSON"],
            horizontal=True,
            index=0,
            key="ca_px_nb_view",
        )
        if view_mode == "Rendered":
            _render_notebook(path)
        else:
            raw, truncated = _read_text_safely(path)
            if truncated:
                st.warning(
                    f"Preview tronqu√©e √† {_fmt_bytes(MAX_PREVIEW_BYTES)} (notebook trop volumineux).",
                    icon="‚ö†Ô∏è",
                )
            st.code(raw, language="json")
    else:
        try:
            content, truncated = _read_text_safely(path)
            if truncated:
                st.warning(
                    f"Preview tronqu√©e √† {_fmt_bytes(MAX_PREVIEW_BYTES)} (fichier volumineux).",
                    icon="‚ö†Ô∏è",
                )
            lang = _language_for(path)
            st.code(content, language=lang)
        except Exception as e:
            st.error(f"Erreur de lecture : {e}", icon="‚ùå")

with right:
    _render_doc_panel_manual(choice, path)


### FILE: pages\6_üìà_Portfolio_Tracking.py
# pages/6_üìà_Portfolio_Tracking.py
import numpy as np
import pandas as pd
import streamlit as st

from app_lib.style import apply_page_config, apply_css
from app_lib.state import data_dir, sidebar_run_selector, require_outdir
from app_lib.io import (
    list_runs, load_totals, load_portfolio_table,
    # load_totals_timeseries, load_cpty_metric_across_runs  # removed
)

apply_page_config(title="Portfolio Tracking ‚Äî XVA", icon="üìà")
apply_css()

DATA = data_dir()
runs = list_runs(str(DATA))

outdir = sidebar_run_selector(runs)
require_outdir(outdir)

st.title("üìà Portfolio Tracking")
st.caption("Ranking, deltas run-vs-run, historique des runs, export CSV.")

# -----------------------------
# Totals (current run)
# -----------------------------
totals = load_totals(outdir)
cva_total = float(totals.get("CVA_total", np.nan))
dva_total = float(totals.get("DVA_total", np.nan))
net_total = dva_total - cva_total if np.isfinite(cva_total) and np.isfinite(dva_total) else np.nan

c1, c2, c3 = st.columns(3)
c1.metric("CVA total", f"{cva_total:,.6g}")
c2.metric("DVA total", f"{dva_total:,.6g}")
c3.metric("Net (DVA ‚àí CVA)", f"{net_total:,.6g}")

st.divider()

# -----------------------------
# Ranking table (current run)
# -----------------------------
st.subheader("üèÅ Portfolio ranking (par contrepartie)")

df = load_portfolio_table(str(outdir))
if df.empty:
    st.warning("Impossible de construire la table portfolio (meta_*.json manquants ?).")
    st.stop()

left, right = st.columns([1.1, 0.9])

with left:
    search = st.text_input("Recherche cid", value="")
    metric = st.radio(
        "Classer par",
        ["CVA", "DVA", "Net_DVA_minus_CVA", "Abs_Net", "Gross_CVA_plus_DVA"],
        horizontal=True,
        index=0,
    )
with right:
    topn = st.slider("Top N", min_value=2, max_value=min(50, len(df)), value=min(20, len(df)))
    ascending = st.checkbox("Tri ascendant", value=False)

df_view = df.copy()
if search.strip():
    df_view = df_view[df_view["cid"].str.contains(search.strip(), case=False, na=False)]

df_view = df_view.sort_values(metric, ascending=ascending).reset_index(drop=True)

st.dataframe(
    df_view[["cid", "CVA", "DVA", "Net_DVA_minus_CVA", "Gross_CVA_plus_DVA", "Abs_Net", "LGD_cpty"]],
    use_container_width=True,
    height=420,
)

# Charts top N
st.markdown("#### üìä Top movers (current run)")
df_top = df_view.head(topn)

cA, cB = st.columns(2)
with cA:
    st.write(f"Top {topn} ‚Äî {metric}")
    chart = df_top.set_index("cid")[metric]
    st.bar_chart(chart)

with cB:
    st.write("Scatter CVA vs DVA")
    df_sc = df_view[["cid", "CVA", "DVA"]].copy()
    st.dataframe(df_sc.head(topn), use_container_width=True)

csv_bytes = df_view.to_csv(index=False).encode("utf-8")
st.download_button(
    "‚¨áÔ∏è T√©l√©charger ranking CSV",
    data=csv_bytes,
    file_name=f"{outdir.name}_portfolio_ranking.csv",
    use_container_width=True,
)

st.divider()

# -----------------------------
# Compare run vs run (deltas)
# -----------------------------
st.subheader("üîÅ Compare run vs run (Œî par contrepartie)")

other_runs = [r for r in runs if r != outdir.name]
if not other_runs:
    st.info("Aucun autre run disponible pour comparer.")
else:
    compare_run = st.selectbox("Run de comparaison", other_runs, index=0)
    outdir2 = DATA / compare_run

    df2 = load_portfolio_table(str(outdir2))
    if df2.empty:
        st.warning("Le run de comparaison ne contient pas meta_*.json.")
    else:
        merged = df.merge(df2, on="cid", how="inner", suffixes=("_base", "_cmp"))
        merged["dCVA"] = merged["CVA_cmp"] - merged["CVA_base"]
        merged["dDVA"] = merged["DVA_cmp"] - merged["DVA_base"]
        merged["dNet"] = merged["Net_DVA_minus_CVA_cmp"] - merged["Net_DVA_minus_CVA_base"]
        merged["abs_dCVA"] = merged["dCVA"].abs()
        merged["abs_dDVA"] = merged["dDVA"].abs()

        col1, col2, col3 = st.columns(3)
        col1.metric("ŒîCVA (sum)", f"{merged['dCVA'].sum():,.6g}")
        col2.metric("ŒîDVA (sum)", f"{merged['dDVA'].sum():,.6g}")
        col3.metric("ŒîNet (sum)", f"{merged['dNet'].sum():,.6g}")

        sort_key = st.radio("Trier deltas par", ["abs_dCVA", "abs_dDVA", "dCVA", "dDVA", "dNet"], horizontal=True)
        merged = merged.sort_values(sort_key, ascending=False)

        st.dataframe(
            merged[[
                "cid",
                "CVA_base", "CVA_cmp", "dCVA",
                "DVA_base", "DVA_cmp", "dDVA",
                "Net_DVA_minus_CVA_base", "Net_DVA_minus_CVA_cmp", "dNet",
                "LGD_cpty_base"
            ]],
            use_container_width=True,
            height=420
        )

        st.download_button(
            "‚¨áÔ∏è T√©l√©charger deltas CSV",
            data=merged.to_csv(index=False).encode("utf-8"),
            file_name=f"{outdir.name}_vs_{compare_run}_deltas.csv",
            use_container_width=True,
        )


### FILE: pages\code_docs.py
# pages/code_docs.py  (ou streamlit_app/ui/code_docs.py selon ton import)
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional

import streamlit as st

# Root project (same logic as your Project Explorer)
ROOT = Path(__file__).resolve().parents[1]
REGISTRY_PATH = ROOT / "pages" / "docs_registry.json"


def load_docs_registry() -> Dict[str, Any]:
    """
    Loads manual docs registry (JSON) keyed by file relpath.
    Cached because Streamlit reruns a lot.
    """
    @st.cache_data(show_spinner=False)
    def _load(p: str) -> Dict[str, Any]:
        path = Path(p)
        if not path.exists():
            return {}
        try:
            return json.loads(path.read_text(encoding="utf-8"))
        except Exception:
            return {}

    return _load(str(REGISTRY_PATH))


def manual_doc_for(relpath: str) -> Optional[Dict[str, Any]]:
    reg = load_docs_registry()
    d = reg.get(relpath, None)
    return d if isinstance(d, dict) else None


def render_doc_panel(relpath: str, path: Path) -> None:
    """
    Right-hand panel: manual docs only.
    (No auto-doc / AST.)
    """
    st.subheader("Documentation")

    manual = manual_doc_for(relpath)
    if manual is None:
        st.info(
            "Pas de fiche manuelle pour ce fichier. "
            "Ajoute une entr√©e dans `docs_registry.json` pour l‚Äôafficher ici.",
            icon="‚ÑπÔ∏è",
        )
        return

    title = manual.get("title", relpath)
    tags = manual.get("tags", [])
    summary = manual.get("summary", "")
    usage = manual.get("usage", "")
    notes = manual.get("notes", "")

    st.markdown(f"### {title}")
    if tags:
        st.caption(" ‚Ä¢ ".join([f"`{t}`" for t in tags]))

    if summary:
        st.markdown(summary)

    if usage:
        st.markdown("#### Usage")
        st.markdown(usage)

    if notes:
        st.markdown("#### Notes")
        st.markdown(notes)


### FILE: src\test.py
import numpy as np

from src.core.timegrid import TimeGrid
from src.rates.termstructure.nelson_siegel import NelsonSiegel
from src.rates.hw1f import HW1FModel
from src.rates.zc_pricer import ZCAnalyticHW
from src.rates.df_curve import DFCurveOnGrid
from src.products.schedule import Schedule
from src.products.swap import Swap
from src.credit.entities import Counterparty, Bank
from src.sim.rng import RNG
from src.sim.scenario_engine import Simulator

# Courbe & HW++
ns = NelsonSiegel(beta0=0.02, beta1=-0.01, beta2=-0.02, tau=2.5)
grid = TimeGrid(T=5.0, dt=1/12)
hw = HW1FModel(kappa=0.60, sigma=0.01); hw.fit_theta_to_curve(ns, grid)
zc = ZCAnalyticHW(hw, ns)

# DF sur grille
df_on_grid = DFCurveOnGrid(ts=ns, grid=grid)

# Banque (LGD=60%, spread 80 bps)
bank = Bank.from_spread(LGD=0.60, spread0=0.0080, kappa_lambda=1.0, sigma_lambda=0.35)

# 2 contreparties pour test (payer/receiver, off-par ¬±25 bps)
sched = Schedule(0.0, 5.0, 2)
# par rate √† t=0
r0 = ns.inst_forward(0.0)
swap_tmp = Swap(notional=1_000_000, direction="payer_fix", coupon=0.0, schedule=sched)
Kpar = swap_tmp.par_rate(0.0, r0, zc)

c1 = Counterparty.from_spread(
    cid="CPTY_01", LGD=0.45, spread0=0.0150, kappa_lambda=1.0, sigma_lambda=0.35,
    swap=Swap(notional=1_000_000, direction="payer_fix",    coupon=Kpar+0.0025, schedule=sched)
)
c2 = Counterparty.from_spread(
    cid="CPTY_02", LGD=0.40, spread0=0.0200, kappa_lambda=1.2, sigma_lambda=0.40,
    swap=Swap(notional=2_000_000, direction="receiver_fix", coupon=Kpar-0.0025, schedule=sched)
)

# RNG + Simulator
rng = RNG(seed=123).gen
sim = Simulator(grid=grid, rate_model=hw, zc_pricer=zc, bank=bank, rng=rng, df_curve_on_grid=df_on_grid)

out = sim.run_portfolio([c1, c2], N=5000)
print("Totals:", out["totals"]["CVA_total"], out["totals"]["DVA_total"])
print("Legs shapes:", out["totals"]["CVA_legs_sum"].shape, out["totals"]["DVA_legs_sum"].shape)
print("Per-cpty count:", len(out["per_counterparty"]))


### FILE: src\__init__.py


### FILE: src\core\timegrid.py
"""
Time grid utilities.

We represent time in *years* (ACT/365 or 30/360 choice comes earlier when you
build T and dt). The grid is uniform for simplicity: t_k = k * dt, k=0..K.
"""

from __future__ import annotations
from dataclasses import dataclass
import numpy as np


@dataclass(frozen=True)
class TimeGrid:
    """
    Uniform time grid on [0, T] with step dt (years).

    Attributes
    ----------
    T : float
        Horizon in years (e.g., 5.0 for 5Y).
    dt : float
        Time step in years (e.g., 1/12 for monthly).
    times : np.ndarray
        Array of shape (K+1,) with t_k = k*dt, last point == T (within tol).
    """
    T: float
    dt: float

    def __post_init__(self) -> None:
        if self.T <= 0:
            raise ValueError("TimeGrid: T must be > 0")
        if self.dt <= 0:
            raise ValueError("TimeGrid: dt must be > 0")
        # Compute number of steps with safe rounding
        K_float = self.T / self.dt
        K = int(round(K_float))
        if abs(K_float - K) > 1e-10:
            # Allow tiny floating mismatch, but warn users by being explicit.
            # You can make this a warning if you prefer.
            raise ValueError(
                f"TimeGrid: T/dt must be (almost) integer. Got T/dt={K_float:.12f}"
            )
        object.__setattr__(self, "_K", K)
        times = np.linspace(0.0, self.T, K + 1, dtype=float)
        object.__setattr__(self, "times", times)

        # Safety: ensure last time equals T to numerical tolerance
        if abs(self.times[-1] - self.T) > 1e-12:
            raise AssertionError("TimeGrid: last time is not equal to T (num issue)")

    @property
    def K(self) -> int:
        """Number of steps (so there are K+1 time points including t0=0)."""
        return self._K

    def index_of_time(self, t: float) -> int:
        """
        Return k such that times[k] == t within tolerance, else raise.
        Useful when aligning product cashflows to grid points.
        """
        idx = int(round(t / self.dt))
        if idx < 0 or idx > self.K:
            raise IndexError(f"time {t} out of grid [0,{self.T}]")
        if abs(self.times[idx] - t) > 1e-10:
            raise ValueError(f"time {t} is not on the grid (dt={self.dt})")
        return idx

    def nearest_index(self, t: float) -> int:
        """Return argmin |times - t| (no requirement that t lies on grid)."""
        return int(np.argmin(np.abs(self.times - t)))

    def contains(self, t: float) -> bool:
        """True if t in [0, T] (inclusive)."""
        return (t >= -1e-14) and (t <= self.T + 1e-14)


### FILE: src\core\utils.py
"""
Utilities: RNG, numeric helpers, sanity checks.

Intended to be lightweight and dependency-free (NumPy only).
"""

from __future__ import annotations
import math
from dataclasses import dataclass
from typing import Iterable, Tuple

import numpy as np


# ===== Randomness / Seeds =====================================================

def make_rng(seed: int | None = None) -> np.random.Generator:
    """
    Create a NumPy Generator with an optional seed.
    Use ONE rng per run and pass it explicitly to all simulators.
    """
    if seed is None:
        return np.random.default_rng()
    return np.random.default_rng(seed)


# ===== Numerics ===============================================================

def trapz_increment(y_prev: float, y_curr: float, dt: float) -> float:
    """
    Trapezoidal increment for ‚à´ y dt over a single step [t, t+dt].
    Returns 0.5 * (y_prev + y_curr) * dt
    """
    return 0.5 * (y_prev + y_curr) * dt


def trapz(x: np.ndarray, y: np.ndarray) -> float:
    """
    Trapezoidal rule for ‚à´ y(x) dx over the full grid.
    Thin wrapper over numpy.trapz with shape/type assertions.
    """
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=float)
    if x.ndim != 1 or y.ndim != 1:
        raise ValueError("trapz expects 1D arrays")
    if x.shape[0] != y.shape[0]:
        raise ValueError("trapz: x and y must have same length")
    return float(np.trapz(y, x))


# ===== Sanity checks / assertions ============================================

def assert_finite(name: str, arr: np.ndarray) -> None:
    """Raise if any NaN/Inf in arr."""
    arr = np.asarray(arr)
    if not np.isfinite(arr).all():
        raise ValueError(f"{name}: contains NaN/Inf")


def assert_shape(name: str, arr: np.ndarray, expected: Tuple[int, ...]) -> None:
    """
    Assert exact shape. Use -1 as wildcard for any length on that axis.
    Example: assert_shape("rates", rates, (N, -1)) where rates.shape==(N,K)
    """
    arr = np.asarray(arr)
    if arr.ndim != len(expected):
        raise ValueError(f"{name}: expected {len(expected)}D, got {arr.ndim}D")
    for i, (got, exp) in enumerate(zip(arr.shape, expected)):
        if exp != -1 and got != exp:
            raise ValueError(f"{name}: axis {i} expected {exp}, got {got}")


def is_monotone_decreasing(x: Iterable[float]) -> bool:
    """True if strictly non-increasing (allows equal)."""
    prev = None
    for v in x:
        if prev is not None and v > prev + 1e-14:
            return False
        prev = v
    return True


# ===== Small helpers ==========================================================

@dataclass(frozen=True)
class Stats1D:
    mean: float
    std: float
    min: float
    max: float

def stats1d(a: np.ndarray) -> Stats1D:
    """Quick descriptive stats for logging/debug."""
    a = np.asarray(a, dtype=float).ravel()
    return Stats1D(
        mean=float(np.mean(a)),
        std=float(np.std(a, ddof=0)),
        min=float(np.min(a)),
        max=float(np.max(a)),
    )


### FILE: src\core\__init__.py


### FILE: src\credit\entities.py
"""
Credit entities (Counterparty, Bank) holding LGD, spread0, and log-OU params.

We calibrate lambda_bar from spread0 and LGD, and compute theta accordingly:
    lambda_bar = spread0 / LGD
    theta = log(lambda_bar) - sigma^2/(4*kappa)

x0 is set to log(lambda_bar) by default (consistent initialization).
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Optional

import numpy as np

from .log_ou_intensity import LogOUIntensity
from ..products.swap import Swap


@dataclass
class CreditParams:
    kappa_lambda: float
    sigma_lambda: float
    theta_lambda: float
    x0: float


def build_credit_params_from_spread(
    spread0: float, LGD: float, kappa_lambda: float, sigma_lambda: float
) -> CreditParams:
    if LGD <= 0 or LGD > 1:
        raise ValueError("LGD must be in (0,1]")
    if spread0 < 0:
        raise ValueError("spread0 must be >= 0")
    lambda_bar = spread0 / LGD
    theta = LogOUIntensity.theta_from_lambda_bar(lambda_bar, kappa_lambda, sigma_lambda)
    x0 = float(np.log(lambda_bar))
    return CreditParams(
        kappa_lambda=kappa_lambda,
        sigma_lambda=sigma_lambda,
        theta_lambda=theta,
        x0=x0,
    )


@dataclass
class Counterparty:
    cid: str
    LGD: float
    spread0: float              # in decimal per year (e.g., 0.015 for 150 bps)
    kappa_lambda: float
    sigma_lambda: float
    theta_lambda: float
    x0: float
    swap: Swap                  # product driving exposure

    @classmethod
    def from_spread(
        cls,
        cid: str,
        LGD: float,
        spread0: float,
        kappa_lambda: float,
        sigma_lambda: float,
        swap: Swap,
    ) -> "Counterparty":
        cp = build_credit_params_from_spread(spread0, LGD, kappa_lambda, sigma_lambda)
        return cls(
            cid=cid,
            LGD=LGD,
            spread0=spread0,
            kappa_lambda=cp.kappa_lambda,
            sigma_lambda=cp.sigma_lambda,
            theta_lambda=cp.theta_lambda,
            x0=cp.x0,
            swap=swap,
        )

    def make_model(self) -> LogOUIntensity:
        return LogOUIntensity(
            kappa=self.kappa_lambda,
            sigma=self.sigma_lambda,
            theta=self.theta_lambda,
            x0=self.x0,
        )


@dataclass
class Bank:
    LGD: float
    spread0: float
    kappa_lambda: float
    sigma_lambda: float
    theta_lambda: float
    x0: float

    @classmethod
    def from_spread(
        cls, LGD: float, spread0: float, kappa_lambda: float, sigma_lambda: float
    ) -> "Bank":
        cp = build_credit_params_from_spread(spread0, LGD, kappa_lambda, sigma_lambda)
        return cls(
            LGD=LGD,
            spread0=spread0,
            kappa_lambda=cp.kappa_lambda,
            sigma_lambda=cp.sigma_lambda,
            theta_lambda=cp.theta_lambda,
            x0=cp.x0,
        )

    def make_model(self) -> LogOUIntensity:
        return LogOUIntensity(
            kappa=self.kappa_lambda,
            sigma=self.sigma_lambda,
            theta=self.theta_lambda,
            x0=self.x0,
        )


### FILE: src\credit\log_ou_intensity.py
"""
Log-OU intensity model for default risk:

    dx_t = kappa*(theta - x_t) dt + sigma dZ_t,   lambda_t = exp(x_t) > 0

We simulate x on a uniform grid using the *exact Gaussian transition* of OU,
then map to lambda, integrate by trapezoid to get survival S and marginal PD.

Shapes:
- returns lambda_, S, PD with shapes (N, K+1), (N, K+1), (N, K+1)
  where PD[:,0] = 0 and for k>=1, PD[:,k] = S[:,k-1] - S[:,k].
"""

from __future__ import annotations
from dataclasses import dataclass
import numpy as np

from ..core.timegrid import TimeGrid


@dataclass
class LogOUIntensity:
    kappa: float      # speed of mean reversion
    sigma: float      # vol of x = log(lambda)
    theta: float      # long-run mean of x
    x0: float         # initial x(0) = log(lambda_0)

    def __post_init__(self) -> None:
        if self.kappa <= 0:
            raise ValueError("LogOUIntensity: kappa must be > 0")
        if self.sigma < 0:
            raise ValueError("LogOUIntensity: sigma must be >= 0")

    @staticmethod
    def theta_from_lambda_bar(lambda_bar: float, kappa: float, sigma: float) -> float:
        """
        Calibrate theta so that E[lambda_t] (long-run) ‚âà lambda_bar:
            E[lambda]_{‚àû} = exp( theta + sigma^2 / (4 kappa) )  =>  theta = log(lambda_bar) - sigma^2/(4 kappa)
        """
        if lambda_bar <= 0:
            raise ValueError("lambda_bar must be > 0")
        return float(np.log(lambda_bar) - (sigma * sigma) / (4.0 * kappa))

    def simulate(self, N: int, grid: TimeGrid, rng: np.random.Generator) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Simulate lambda, survival S, and marginal PD on the provided grid.

        Returns
        -------
        lambda_ : (N, K+1)
        S       : (N, K+1), with S[:,0] = 1
        PD      : (N, K+1), with PD[:,0] = 0 and PD[:,k] = S[:,k-1] - S[:,k]
        """
        a, sig = self.kappa, self.sigma
        times = grid.times
        K = grid.K

        x = np.empty((N, K + 1), dtype=float)
        lam = np.empty_like(x)
        S = np.empty_like(x)
        PD = np.zeros_like(x)

        # init
        x[:, 0] = self.x0
        lam[:, 0] = np.exp(x[:, 0])
        S[:, 0] = 1.0

        # precompute OU variance for each step (uniform grid -> constant)
        dt = times[1] - times[0]  # uniform by construction
        e2a = np.exp(-2.0 * a * dt)
        ead = np.exp(-a * dt)
        var_step = (sig * sig) * (1.0 - e2a) / (2.0 * a)  # Var[x_{k+1} | x_k]
        std_step = np.sqrt(var_step)

        # cumulative integral A_k = ‚à´_0^{t_k} lambda_s ds (trapz incremental)
        A = np.zeros(N, dtype=float)

        for k in range(K):
            # OU exact transition for x
            mean_next = self.theta + (x[:, k] - self.theta) * ead
            Z = rng.standard_normal(size=N)
            x[:, k + 1] = mean_next + std_step * Z

            # map to lambda
            lam[:, k + 1] = np.exp(x[:, k + 1])

            # trapezoid increment for ‚à´ lambda ds over [t_k, t_{k+1}]
            A += 0.5 * (lam[:, k] + lam[:, k + 1]) * dt
            S[:, k + 1] = np.exp(-A)

            # marginal PD on (t_k, t_{k+1}]
            PD[:, k + 1] = S[:, k] - S[:, k + 1]

        return lam, S, PD


### FILE: src\credit\__init__.py


### FILE: src\exposure\exposure_engine.py
"""
Exposure engine for vanilla IRS (no CSA).

Given:
- rate paths r[n,k] on grid t_k (HW1F++),
- a Swap (notional, direction, coupon, schedule),
- a ZCAnalyticHW pricer (providing A(t,T), B(t,T), P(t,T)=A*exp(-B*r_t)),

we compute scenario-wise MTM V_{n,k}, then EPE_k = mean(max(V,0)), ENE_k = mean(max(-V,0)).

Vectorization strategy
----------------------
For each grid time t_k, we precompute the affine bond coefficients (A_kj, B_kj)
for all remaining payment dates T_j > t_k, plus:
- (A_prev_k, B_prev_k) for the "previous" coupon date T_prev (or t_start),
- (A_last_k, B_last_k) for the final maturity T_last.

Then for all scenarios n:
    P_kj(n) = A_kj * exp(-B_kj * r[n,k])   # broadcasts over j
    Annuity_k(n) = sum_j alpha_j * P_kj(n)
    Float_k(n)   = P(t_k, T_prev_k) - P(t_k, T_last)

Finally:
    V_k(n) = N * [ Float_k(n) - K * Annuity_k(n) ]   (payer_fix)
           = N * [ K * Annuity_k(n) - Float_k(n) ]   (receiver_fix)
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np

from ..core.timegrid import TimeGrid
from ..rates.zc_pricer import ZCAnalyticHW
from ..products.swap import Swap


@dataclass
class _CoeffAtTime:
    # For remaining cashflows at t_k:
    A_pay: np.ndarray   # shape (m_k,)   for T_j > t_k
    B_pay: np.ndarray   # shape (m_k,)
    alpha: np.ndarray   # shape (m_k,)   accruals corresponding to those T_j
    # For float leg:
    A_prev: float       # for T_prev (or t_start‚âàt with P=1)
    B_prev: float
    A_last: float       # for T_last
    B_last: float


class ExposureEngine:
    def __init__(self, zc: ZCAnalyticHW) -> None:
        self.zc = zc
        self._cache: Dict[Tuple[float, Tuple[float, ...]], _CoeffAtTime] = {}

    # ---------- Precompute A,B coeffs for speed (per t_k) ---------------------

    def _precompute_coeffs_for_time(self, t: float, swap: Swap) -> _CoeffAtTime:
        """
        Build the A,B arrays for remaining fixed-leg dates T_j > t and the scalars for
        the float leg using the robust approximation:
            Float(t) ‚âà 1 - P(t, T_last)
        i.e. set P_prev(t) = 1.0  (A_prev=1, B_prev=0) for all t.

        This avoids calling A(t, T_prev) with T_prev < t (undefined in our pricer),
        and is accurate enough for monthly EPE/ENE buckets.
        """
        # Remaining fixed leg (T_j > t)
        T_pay, alpha = swap.schedule.remaining_after(t)
        if T_pay.size:
            A_pay = np.array([self.zc.A(t, T) for T in T_pay], dtype=float)
            B_pay = np.array([self.zc.B(t, T) for T in T_pay], dtype=float)
        else:
            A_pay = np.array([], dtype=float)
            B_pay = np.array([], dtype=float)

        # Float leg: use Float(t) ‚âà 1 - P(t, T_last)
        pay_times = swap.schedule.pay_times
        if pay_times.size == 0:
            # No cashflows at all
            A_prev = 1.0; B_prev = 0.0
            A_last = 1.0; B_last = 0.0
        else:
            T_last = float(pay_times[-1])

            # IMPORTANT: if t is after maturity, exposure is 0 ‚Üí set P_last = 1
            if T_last <= t + 1e-14:
                A_prev = 1.0; B_prev = 0.0
                A_last = 1.0; B_last = 0.0
            else:
                A_last = self.zc.A(t, T_last)
                B_last = self.zc.B(t, T_last)
                A_prev = 1.0
                B_prev = 0.0

        return _CoeffAtTime(
            A_pay=A_pay,
            B_pay=B_pay,
            alpha=alpha,
            A_prev=A_prev,
            B_prev=B_prev,
            A_last=A_last,
            B_last=B_last,
        )

    def _coeffs(self, t: float, swap: Swap) -> _CoeffAtTime:
        key = (t, tuple(swap.schedule.pay_times.tolist()))
        c = self._cache.get(key)
        if c is None:
            c = self._precompute_coeffs_for_time(t, swap)
            self._cache[key] = c
        return c

    # ---------- Core API: EPE/ENE from rate paths ----------------------------

    def epe_ene(self, rates: np.ndarray, grid: TimeGrid, swap: Swap) -> tuple[np.ndarray, np.ndarray]:
        """
        Compute EPE/ENE on the grid for a single swap given simulated short-rate paths.

        Parameters
        ----------
        rates : (N, K+1) array of short rates r_{n,k}
        grid  : TimeGrid
        swap  : Swap

        Returns
        -------
        EPE : (K+1,) expected positive exposure per grid time
        ENE : (K+1,) expected negative exposure per grid time
        """
        rates = np.asarray(rates, dtype=float)
        N, Kp1 = rates.shape
        K = Kp1 - 1

        EPE = np.zeros(Kp1, dtype=float)
        ENE = np.zeros(Kp1, dtype=float)

        notional = swap.notional
        K_coupon = swap.coupon
        payer = (swap.direction == "payer_fix")

        times = grid.times

        for k in range(Kp1):
            t = float(times[k])

            # ---- SPECIAL CASE t=0: utiliser zc.P(0, r0, T) pour un recollage exact
            if k == 0:
                # Jambe fixe restante √† t=0 (d√©terministe)
                T_pay, alpha = swap.schedule.remaining_after(t)
                if T_pay.size == 0:
                    annuity_n = np.zeros(N, dtype=float)
                else:
                    # P(0,T) identique pour tous les sc√©narios
                    P0 = np.array([self.zc.P(0.0, rates[0, 0], T) for T in T_pay], dtype=float)
                    A0 = float(np.dot(alpha, P0))
                    annuity_n = np.full(N, A0, dtype=float)

                # Jambe flottante ‚âà 1 - P(0, T_last)
                pay_times = swap.schedule.pay_times
                if pay_times.size == 0:
                    float_n = np.zeros(N, dtype=float)
                else:
                    T_last = float(pay_times[-1])
                    P_last0 = self.zc.P(0.0, rates[0, 0], T_last)
                    float_n = np.full(N, 1.0 - P_last0, dtype=float)

                # MTM et expositions
                if payer:
                    v_n = notional * (float_n - K_coupon * annuity_n)
                else:
                    v_n = notional * (K_coupon * annuity_n - float_n)

                Epos = np.maximum(v_n, 0.0)
                Eneg = np.maximum(-v_n, 0.0)
                EPE[k] = float(Epos.mean())
                ENE[k] = float(Eneg.mean())
                continue  # prochain k

            # ---- Cas g√©n√©rique t>0 : version vectoris√©e avec A,B pr√©-calcul√©s
            coeffs = self._coeffs(t, swap)

            # Jambe fixe (annuity)
            if coeffs.A_pay.size == 0:
                annuity_n = np.zeros(N, dtype=float)
            else:
                exp_term = np.exp(-np.outer(rates[:, k], coeffs.B_pay))  # (N, m_k)
                P_kj = coeffs.A_pay[None, :] * exp_term                  # (N, m_k)
                annuity_n = P_kj @ coeffs.alpha                          # (N,)

            # Jambe flottante ‚âà 1 - P(t, T_last) (on a A_prev=1, B_prev=0 dans _precompute_)
            if coeffs.B_prev == 0.0:
                P_prev = np.ones(N, dtype=float)  # =1
            else:
                P_prev = coeffs.A_prev * np.exp(-coeffs.B_prev * rates[:, k])

            P_last = coeffs.A_last * np.exp(-coeffs.B_last * rates[:, k])
            float_n = P_prev - P_last

            # MTM et expositions
            if payer:
                v_n = notional * (float_n - K_coupon * annuity_n)
            else:
                v_n = notional * (K_coupon * annuity_n - float_n)

            Epos = np.maximum(v_n, 0.0)
            Eneg = np.maximum(-v_n, 0.0)
            EPE[k] = float(Epos.mean())
            ENE[k] = float(Eneg.mean())

        return EPE, ENE

    def mtm_paths(self, rates: np.ndarray, grid: TimeGrid, swap: Swap) -> np.ndarray:
        """
        Return V[n,k] for audit/plots (no netting, no CSA).
        """
        rates = np.asarray(rates, dtype=float)
        N, Kp1 = rates.shape
        times = grid.times

        V = np.zeros((N, Kp1), dtype=float)
        payer = (swap.direction == "payer_fix")
        notional = swap.notional
        K_coupon = swap.coupon

        for k in range(Kp1):
            t = float(times[k])

            if k == 0:
                # Jambe fixe √† t=0 (d√©terministe)
                T_pay, alpha = swap.schedule.remaining_after(t)
                if T_pay.size == 0:
                    annuity_n = np.zeros(N, dtype=float)
                else:
                    P0 = np.array([self.zc.P(0.0, rates[0, 0], T) for T in T_pay], dtype=float)
                    A0 = float(np.dot(alpha, P0))
                    annuity_n = np.full(N, A0, dtype=float)

                # Jambe flottante ‚âà 1 - P(0, T_last)
                pay_times = swap.schedule.pay_times
                if pay_times.size == 0:
                    float_n = np.zeros(N, dtype=float)
                else:
                    T_last = float(pay_times[-1])
                    P_last0 = self.zc.P(0.0, rates[0, 0], T_last)
                    float_n = np.full(N, 1.0 - P_last0, dtype=float)

                if payer:
                    V[:, k] = notional * (float_n - K_coupon * annuity_n)
                else:
                    V[:, k] = notional * (K_coupon * annuity_n - float_n)
                continue

            # t>0 : version vectoris√©e
            coeffs = self._coeffs(t, swap)

            if coeffs.A_pay.size == 0:
                annuity_n = np.zeros(N, dtype=float)
            else:
                P_kj = coeffs.A_pay[None, :] * np.exp(-np.outer(rates[:, k], coeffs.B_pay))
                annuity_n = P_kj @ coeffs.alpha

            if coeffs.B_prev == 0.0:
                P_prev = np.ones(N, dtype=float)  # =1
            else:
                P_prev = coeffs.A_prev * np.exp(-coeffs.B_prev * rates[:, k])

            P_last = coeffs.A_last * np.exp(-coeffs.B_last * rates[:, k])
            float_n = P_prev - P_last

            if payer:
                V[:, k] = notional * (float_n - K_coupon * annuity_n)
            else:
                V[:, k] = notional * (K_coupon * annuity_n - float_n)

        return V


### FILE: src\exposure\__init__.py


### FILE: src\io\config.py
# src/io/config.py
from __future__ import annotations
import pathlib
import yaml

def load_settings(path: str | pathlib.Path) -> dict:
    """
    Charge le fichier YAML de configuration et renvoie un dict.
    Utilise yaml.safe_load. L√®ve FileNotFoundError si absent.
    """
    p = pathlib.Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Config file not found: {p}")
    with p.open("r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    # validations minimales
    required = ["seed", "horizon_years", "dt_years", "n_scenarios",
                "rates", "discount_curve", "credit_defaults", "bank", "portfolio"]
    for k in required:
        if k not in cfg:
            raise ValueError(f"Missing required key in settings: '{k}'")
    return cfg


### FILE: src\io\outputs.py
"""
CSV/JSON exports for the XVA simulation.

We keep it dependency-free (no pandas). All arrays are written as simple CSVs.
Directory is created if missing.
"""

from __future__ import annotations
import os, csv, json
from dataclasses import dataclass
from typing import Any, Dict, List
import numpy as np


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def write_vector_csv(filepath: str, header: str, vec: np.ndarray) -> None:
    _ensure_dir(os.path.dirname(filepath))
    vec = np.asarray(vec, dtype=float).ravel()
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([header])
        for v in vec:
            w.writerow([f"{v:.12g}"])


def write_matrix_csv(filepath: str, headers: List[str], rows: List[List[Any]]) -> None:
    _ensure_dir(os.path.dirname(filepath))
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(headers)
        for row in rows:
            w.writerow(row)


def write_meta_json(filepath: str, meta: Dict[str, Any]) -> None:
    _ensure_dir(os.path.dirname(filepath))
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)


# ---------- High-level exports -----------------------------------------------

def export_per_counterparty_tables(outdir: str, per_cpty: List[Dict[str, Any]]) -> None:
    """
    For each counterparty 'res' (from Simulator.run_for_counterparty):
      - exposures_{cid}.csv: t_k, EPE_k, ENE_k
      - credit_{cid}.csv   : t_k, PD_k, S_k
      - xva_{cid}.csv      : t_k, CVA_leg_k, DVA_leg_k, and totals on last row
    """
    _ensure_dir(outdir)

    # times can be taken from lengths (assume uniform grid length)
    Kp1 = len(per_cpty[0]["EPE"])
    # We don't have the grid object here; reconstruct times as 0..K with dt=unknown
    # => we write an index column "k". If you want actual times, pass them in the dict.
    idx = list(range(Kp1))

    for res in per_cpty:
        cid = res["cid"]

        rows_expo = [[k, f"{res['EPE'][k]:.12g}", f"{res['ENE'][k]:.12g}"] for k in idx]
        write_matrix_csv(
            os.path.join(outdir, f"exposures_{cid}.csv"),
            headers=["k", "EPE", "ENE"],
            rows=rows_expo,
        )

        rows_credit = [[k, f"{res['PD_cpty'][k]:.12g}", f"{res['S_cpty'][k]:.12g}"] for k in idx]
        write_matrix_csv(
            os.path.join(outdir, f"credit_{cid}.csv"),
            headers=["k", "PD_cpty", "S_cpty"],
            rows=rows_credit,
        )

        rows_xva = [[k, f"{res['CVA_leg'][k]:.12g}", f"{res['DVA_leg'][k]:.12g}"] for k in idx]
        # add a final total line (with k = "TOTAL")
        rows_xva.append(["TOTAL", f"{res['CVA']:.12g}", f"{res['DVA']:.12g}"])

        write_matrix_csv(
            os.path.join(outdir, f"xva_{cid}.csv"),
            headers=["k", "CVA_leg", "DVA_leg"],
            rows=rows_xva,
        )

        # also dump a tiny meta json per cpty (useful for the report)
        meta = {
            "cid": cid,
            "LGD_cpty": res["LGD_cpty"],
            "LGD_bank": res["LGD_bank"],
            "CVA": res["CVA"],
            "DVA": res["DVA"],
        }
        write_meta_json(os.path.join(outdir, f"meta_{cid}.json"), meta)


def export_totals(outdir: str, totals: Dict[str, Any]) -> None:
    """
    Write:
      - totals.csv          : CVA_total, DVA_total
      - cva_legs_sum.csv    : k, CVA_leg_sum_k
      - dva_legs_sum.csv    : k, DVA_leg_sum_k
      - df_curve.csv        : k, DF_k
    """
    _ensure_dir(outdir)
    write_matrix_csv(
        os.path.join(outdir, "totals.csv"),
        headers=["metric", "value"],
        rows=[["CVA_total", f"{totals['CVA_total']:.12g}"],
              ["DVA_total", f"{totals['DVA_total']:.12g}"]],
    )

    Kp1 = len(totals["CVA_legs_sum"])
    idx = list(range(Kp1))

    rows_cva = [[k, f"{totals['CVA_legs_sum'][k]:.12g}"] for k in idx]
    write_matrix_csv(
        os.path.join(outdir, "cva_legs_sum.csv"),
        headers=["k", "CVA_leg_sum"],
        rows=rows_cva,
    )

    rows_dva = [[k, f"{totals['DVA_legs_sum'][k]:.12g}"] for k in idx]
    write_matrix_csv(
        os.path.join(outdir, "dva_legs_sum.csv"),
        headers=["k", "DVA_leg_sum"],
        rows=rows_dva,
    )

    rows_df = [[k, f"{totals['DF'][k]:.12g}"] for k in idx]
    write_matrix_csv(
        os.path.join(outdir, "df_curve.csv"),
        headers=["k", "DF"],
        rows=rows_df,
    )


def export_everything(outdir: str, portfolio_out: Dict[str, Any]) -> None:
    """
    Convenience: write both per-counterparty files and totals.
    """
    export_per_counterparty_tables(
        os.path.join(outdir, "per_counterparty"),
        portfolio_out["per_counterparty"],
    )
    export_totals(
        os.path.join(outdir, "totals"),
        portfolio_out["totals"],   # <-- argument 'totals' manquant corrig√©
    )
    write_meta_json(
        os.path.join(outdir, "meta.json"),
        portfolio_out["meta"],
    )



### FILE: src\io\plots.py
"""
Plot helpers (PNG) for exposures, credit, and CVA/DVA legs.

Each function saves ONE figure per call and closes it (no memory leak).
"""

from __future__ import annotations
import os
import numpy as np
import matplotlib.pyplot as plt


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def save_exposure_plot(times: np.ndarray, EPE: np.ndarray, ENE: np.ndarray, outpath: str, title: str = "Exposure"):
    _ensure_dir(os.path.dirname(outpath))
    plt.figure(figsize=(8, 4.5))
    plt.plot(times, EPE, label="EPE")
    plt.plot(times, ENE, label="ENE")
    plt.xlabel("Time (years)")
    plt.ylabel("Exposure (currency)")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()


def save_credit_plot(times: np.ndarray, PD: np.ndarray, S: np.ndarray, outpath: str, title: str = "Credit (bucket PD & Survival)"):
    _ensure_dir(os.path.dirname(outpath))
    plt.figure(figsize=(8, 4.5))
    # PD is bucketed at k; show from k=1
    if PD.shape[0] == times.shape[0]:
        t_pd = times
    else:
        t_pd = np.arange(PD.shape[0])
    plt.bar(t_pd[1:], PD[1:], width=(times[1]-times[0]) if times.size>1 else 0.02, alpha=0.4, label="PD (marginal)")
    plt.plot(times, S, label="Survival S(t)")
    plt.xlabel("Time (years)")
    plt.title(title)
    plt.legend(loc="best")
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()


def save_xva_legs_plot(times: np.ndarray, CVA_leg: np.ndarray, DVA_leg: np.ndarray, outpath: str, title: str = "CVA/DVA legs"):
    _ensure_dir(os.path.dirname(outpath))
    plt.figure(figsize=(8, 4.5))
    plt.plot(times, CVA_leg, label="CVA_leg")
    plt.plot(times, DVA_leg, label="DVA_leg")
    plt.xlabel("Time (years)")
    plt.ylabel("Value (currency)")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()


def save_totals_legs_plot(times: np.ndarray, CVA_legs_sum: np.ndarray, DVA_legs_sum: np.ndarray, outpath: str, title: str = "Portfolio CVA/DVA legs (sum)"):
    _ensure_dir(os.path.dirname(outpath))
    plt.figure(figsize=(8, 4.5))
    plt.plot(times, CVA_legs_sum, label="Œ£ CVA_leg")
    plt.plot(times, DVA_legs_sum, label="Œ£ DVA_leg")
    plt.xlabel("Time (years)")
    plt.ylabel("Value (currency)")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.savefig(outpath, dpi=150)
    plt.close()


### FILE: src\io\__init__.py


### FILE: src\products\schedule.py
"""
Cashflow schedule for vanilla IRS.

We keep it simple (year-fraction grid in ACT/365-equivalent years).
If you need business-day calendars, day-counts, etc., you can extend later.
"""

from __future__ import annotations
from dataclasses import dataclass
import numpy as np


@dataclass(frozen=True)
class Schedule:
    """
    Payment schedule between t_start and t_end (years), with fixed frequency.

    Parameters
    ----------
    t_start : float
        Start time in years (e.g., 0.0).
    t_end : float
        End time in years (e.g., 5.0).
    freq_per_year : int
        Payments per year (e.g., 2 for semi-annual, 4 for quarterly).
    """
    t_start: float
    t_end: float
    freq_per_year: int

    def __post_init__(self) -> None:
        if self.t_end <= self.t_start:
            raise ValueError("Schedule: t_end must be > t_start")
        if self.freq_per_year <= 0:
            raise ValueError("Schedule: freq_per_year must be > 0")
        object.__setattr__(self, "_dt", 1.0 / float(self.freq_per_year))
        # Build payment times excluding start, including end
        # e.g. start=0, end=5, semi-annual -> [0.5, 1.0, ..., 5.0]
        n = int(round((self.t_end - self.t_start) / self._dt))
        if abs(self.t_start + n * self._dt - self.t_end) > 1e-10:
            raise ValueError("Schedule: (t_end - t_start) must be multiple of 1/freq")
        pays = self.t_start + self._dt * np.arange(1, n + 1, dtype=float)
        object.__setattr__(self, "_pay_times", pays)
        object.__setattr__(self, "_accruals", np.full_like(pays, self._dt))

    @property
    def pay_times(self) -> np.ndarray:
        """Cashflow payment times T_j (shape (m,))."""
        return self._pay_times

    @property
    def accruals(self) -> np.ndarray:
        """Accrual factors Œ±_j (shape (m,)); here constant dt = 1/freq."""
        return self._accruals

    @property
    def dt(self) -> float:
        """Accrual length (years)."""
        return self._dt

    def remaining_after(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        """
        Times & accruals of remaining payments strictly after time t.
        """
        mask = self._pay_times > (t + 1e-14)
        return self._pay_times[mask], self._accruals[mask]

# ---- ADD: explicit schedule (supports stubs) -------------------------------

@dataclass(frozen=True)
class ExplicitSchedule:
    """
    Schedule d√©fini par pay_times explicites (supporte un 1er stub).
    pay_times: maturit√©s (en ann√©es) depuis l'as-of, strictement croissantes.
    accruals : Œ±_j = pay_times[j] - pay_times[j-1], avec pay_times[-1] apr√®s 0.
    """
    pay_times: np.ndarray
    accruals: np.ndarray

    @classmethod
    def from_pay_times(cls, pay_times) -> "ExplicitSchedule":
        pt = np.asarray(pay_times, dtype=float).ravel()
        if pt.size == 0:
            return cls(pt, pt.copy())
        if pt[0] <= 0.0 or np.any(np.diff(pt) <= 1e-14):
            raise ValueError("ExplicitSchedule: pay_times must be strictly increasing and > 0")
        acc = np.empty_like(pt)
        acc[0] = pt[0]
        acc[1:] = np.diff(pt)
        return cls(pt, acc)

    def remaining_after(self, t: float) -> tuple[np.ndarray, np.ndarray]:
        mask = self.pay_times > (t + 1e-14)
        return self.pay_times[mask], self.accruals[mask]


### FILE: src\products\swap.py
"""
Plain-vanilla fixed-for-float IRS (single currency), priced under HW1F++.

Sign convention:
- direction = "payer_fix": pay fixed, receive float ‚Üí V = Float - K * Annuity
- direction = "receiver_fix": receive fixed, pay float ‚Üí V = K * Annuity - Float
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Literal, Tuple
import numpy as np

from .schedule import Schedule
from ..rates.zc_pricer import ZCAnalyticHW


Direction = Literal["payer_fix", "receiver_fix"]


@dataclass
class Swap:
    notional: float
    direction: Direction
    coupon: float            # K (decimal per year, e.g., 0.025 for 2.5%)
    schedule: Schedule

    # -------- Helpers on remaining legs --------------------------------------

    def _annuity(self, t: float, r_t: float, zc: ZCAnalyticHW) -> float:
        """
        A(t) = sum_{j} Œ±_j * P(t, T_j) over remaining payments T_j > t.
        """
        T, A = self.schedule.remaining_after(t)
        if T.size == 0:
            return 0.0
        P = zc.P_vector(t, r_t, T)
        return float(np.dot(self.schedule.accruals[: P.size], P))

    def _float_leg(self, t: float, r_t: float, zc: ZCAnalyticHW) -> float:
        """
        Value of remaining floating leg at time t (ignores fixing lag/couru),
        standard result under absence of spread: Float(t) = P(t, T_start_next) - P(t, T_maturity).
        Here T_start_next is the last paid date (‚â§ t) replaced by t; with our simple schedule we
        approximate by P(t, t_first_remaining - dt) ‚âà 1 (if t < first payment),
        and more generally we use the standard simplification:

        If t is between two coupon dates and no spread, the PV of float leg equals
        1 - P(t, T_last) only at inception. For general t, the exact expression uses the
        current floating coupon accrual. For EPE/ENE monthly buckets, the common clean
        approximation is:
            Float(t) ‚âà P(t, T_prev) - P(t, T_last)
        where T_prev is the previous coupon date (or t_start).
        We implement a robust version:
            - if t < first payment: Float ‚âà 1 - P(t, T_last)
            - else: use previous schedule date as T_prev.
        """
        pay_times = self.schedule.pay_times
        if pay_times.size == 0:
            return 0.0

        T_last = float(pay_times[-1])
        P_last = zc.P(t, r_t, T_last)

        # Find previous coupon date before t
        prev_idx = np.searchsorted(pay_times, t, side="right") - 1
        if prev_idx < 0:
            # before first coupon ‚Üí approximation 1 - P(t, T_last)
            T_prev = t  # P(t,t)=1
            P_prev = 1.0
        else:
            T_prev = float(pay_times[prev_idx])
            # if T_prev <= t, use discount from t to T_prev (should be <1 but close if near)
            P_prev = zc.P(t, r_t, T_prev)

        return float(P_prev - P_last)

    # -------- Public API ------------------------------------------------------

    def par_rate(self, t: float, r_t: float, zc: ZCAnalyticHW) -> float:
        """
        K_par(t) = Float(t) / Annuity(t), provided Annuity(t) > 0.
        """
        A = self._annuity(t, r_t, zc)
        if A <= 0.0:
            return 0.0
        F = self._float_leg(t, r_t, zc)
        return F / A

    def mtm(self, t: float, r_t: float, zc: ZCAnalyticHW) -> float:
        """
        V(t) = N * [ Float(t) - K * Annuity(t) ]   if payer_fix
             = N * [ K * Annuity(t) - Float(t) ]   if receiver_fix
        """
        A = self._annuity(t, r_t, zc)
        F = self._float_leg(t, r_t, zc)
        if self.direction == "payer_fix":
            v = F - self.coupon * A
        elif self.direction == "receiver_fix":
            v = self.coupon * A - F
        else:
            raise ValueError("Swap.direction must be 'payer_fix' or 'receiver_fix'")
        return self.notional * v

    # Convenience: vectorized MTM across many (t, r_t) pairs (optional)
    def mtm_vector(self, t: float, r_vec: np.ndarray, zc: ZCAnalyticHW) -> np.ndarray:
        """
        Compute MTM for a vector of short rates r_vec at the same time t.
        """
        return np.array([self.mtm(t, r, zc) for r in r_vec], dtype=float)

# ---- ADD: roll/age a swap to a new as-of (t_star) --------------------------

from .schedule import ExplicitSchedule

def roll_swap(swap: "Swap", t_star: float) -> "Swap":
    """
    Revaloriser le M√äME swap √† une nouvelle date as-of t_star:
    on conserve notional/direction/coupon, et on "d√©duis" t_star
    aux dates de paiement futures (T_j > t_star).
    """
    pt = np.asarray(swap.schedule.pay_times, dtype=float)
    mask = pt > (t_star + 1e-14)
    pt_new = pt[mask] - t_star
    sched_new = ExplicitSchedule.from_pay_times(pt_new)
    return Swap(
        notional=swap.notional,
        direction=swap.direction,
        coupon=swap.coupon,
        schedule=sched_new,
    )


### FILE: src\products\__init__.py


### FILE: src\rates\df_curve.py
"""
Build discount factors DF(0, t_k) on a TimeGrid from a TermStructure.

We use the *same* initial term structure as for HW++ (e.g., Nelson‚ÄìSiegel).
"""

from __future__ import annotations
from dataclasses import dataclass
import numpy as np

from .termstructure.base_curve import TermStructure
from ..core.timegrid import TimeGrid


@dataclass
class DFCurveOnGrid:
    ts: TermStructure
    grid: TimeGrid

    def values(self) -> np.ndarray:
        """
        Return DF array of shape (K+1,), DF[k] = DF(0, t_k).
        """
        t = self.grid.times
        df = np.array([self.ts.discount_factor(float(tt)) for tt in t], dtype=float)

        # Safety guards: DF(0)=1, DF decreasing (up to tiny numerical noise)
        df[0] = 1.0
        # enforce weak monotonicity by clipping tiny up-ticks due to floating noise
        for k in range(1, df.size):
            if df[k] > df[k-1] + 1e-14:
                df[k] = df[k-1]

        return df


### FILE: src\rates\discount_curve.py
# src/rates/discount_curve.py
from __future__ import annotations
import math
from typing import Iterable, Sequence, Tuple, Literal, Optional
import numpy as np

class DiscountCurve:
    """
    Courbe d'actualisation initiale DF(0,t).
    Modes:
      - type='flat' : zero rate constant z -> DF(0,t)=exp(-z t)
      - type='points' : liste (t_i, DF_i), interpolation log-lin√©aire sur DF
    """
    def __init__(
        self,
        mode: Literal["flat", "points"],
        flat_zero_rate: Optional[float] = None,
        points: Optional[Sequence[Tuple[float, float]]] = None,
    ):
        self.mode = mode
        if mode == "flat":
            if flat_zero_rate is None:
                raise ValueError("flat_zero_rate required for flat curve")
            if flat_zero_rate < -0.005:  # tol√©rance petits taux n√©gatifs
                raise ValueError("flat_zero_rate seems too negative")
            self.z = float(flat_zero_rate)
            self._t_knots = np.array([0.0])
            self._df_knots = np.array([1.0])
        elif mode == "points":
            if not points:
                raise ValueError("points required for points curve")
            pts = np.array(points, dtype=float)
            if pts.ndim != 2 or pts.shape[1] != 2:
                raise ValueError("points must be sequence of (t, df)")
            # tri par maturit√©
            pts = pts[np.argsort(pts[:, 0])]
            if pts[0, 0] < 0:
                raise ValueError("negative maturities not allowed")
            # ajout (0,1) si absent
            if pts[0, 0] > 0.0:
                pts = np.vstack([[0.0, 1.0], pts])
            # contr√¥les DF
            if not np.all(pts[:, 1] > 0):
                raise ValueError("all discount factors must be > 0")
            self._t_knots = pts[:, 0]
            self._df_knots = pts[:, 1]
        else:
            raise ValueError("mode must be 'flat' or 'points'")

    def df0(self, t: float) -> float:
        """DF(0,t)."""
        if t < 0:
            raise ValueError("t must be >= 0")
        if t == 0:
            return 1.0
        if self.mode == "flat":
            return math.exp(-self.z * t)
        # points: interpolation log-lin√©aire
        t_knots = self._t_knots
        df_knots = self._df_knots
        if t >= t_knots[-1]:
            # extrapolation log-lin√©aire √† partir du dernier segment
            t0, t1 = t_knots[-2], t_knots[-1]
            ln0, ln1 = math.log(df_knots[-2]), math.log(df_knots[-1])
            slope = (ln1 - ln0) / (t1 - t0) if t1 > t0 else 0.0
            ln_df = ln1 + slope * (t - t1)
            return math.exp(ln_df)
        # interpolation dans l'intervalle
        idx = np.searchsorted(t_knots, t)  # position d'insertion
        i0 = idx - 1
        i1 = idx
        t0, t1 = t_knots[i0], t_knots[i1]
        ln0, ln1 = math.log(df_knots[i0]), math.log(df_knots[i1])
        w = (t - t0) / (t1 - t0) if t1 > t0 else 0.0
        ln_df = (1 - w) * ln0 + w * ln1
        return math.exp(ln_df)

    def zero_rate(self, t: float) -> float:
        """
        z(0,t) implicite tel que DF(0,t) = exp(-z t), pour t>0.
        Pour t=0, renvoie z plat si mode flat, sinon 0.
        """
        if t <= 0:
            return self.z if self.mode == "flat" else 0.0
        df = self.df0(t)
        # √©vite division par z√©ro si df‚âà1 et t tr√®s petit
        return -math.log(df) / t


### FILE: src\rates\hw1f.py
"""
Hull‚ÄìWhite 1F '++' model with time-dependent theta(t), fitted to a given
initial term structure through the standard HW++ relation:

    theta(t) = f(0,t) + (1/kappa) * d/dt f(0,t) + (sigma^2 / (2*kappa^2)) * (1 - e^{-2*kappa t})

We provide:
- fit_theta_to_curve(ts, grid) -> callable theta(t)
- simulate_rates(...) : simulate r_t with time-varying mean using a stable OU step
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Callable

import numpy as np

from .termstructure.base_curve import TermStructure
from ..core.timegrid import TimeGrid


@dataclass
class HW1FModel:
    kappa: float
    sigma: float
    # theta_fn will be set after fit to curve; if None, assume flat theta=const
    theta_fn: Callable[[float], float] | None = None

    def __post_init__(self) -> None:
        if self.kappa <= 0:
            raise ValueError("HW1FModel: kappa must be > 0")
        if self.sigma < 0:
            raise ValueError("HW1FModel: sigma must be >= 0")

    # -------- Fit theta(t) to an initial term structure ----------------------

    def fit_theta_to_curve(self, ts: TermStructure, grid: TimeGrid) -> Callable[[float], float]:
        """
        Build Œ∏(t) from the instantaneous forward f(0,t) of the provided term structure,
        using the standard HW++ identity:

            theta(t) = f(0,t) + (1/kappa) * df/dt + (sigma^2 / (2*kappa^2)) * (1 - e^{-2*kappa t})

        We compute df/dt with a robust centered finite difference and clamp near t=0.
        Returns a callable theta(t) to be used both in pricing and in simulation.
        """
        a = self.kappa
        sig = self.sigma

        def df_dt(t: float) -> float:
            # Pas adaptatif : plus petit pr√®s de 0, un peu plus grand ensuite
            # On borne par le bas pour √©viter h=0 en machine.
            base = 1e-6
            h = max(base, 5e-5 * max(1.0, t))
            t_minus = max(0.0, t - h)
            t_plus = t + h
            f_minus = ts.inst_forward(t_minus)
            f_plus = ts.inst_forward(t_plus)
            denom = t_plus - t_minus
            return (f_plus - f_minus) / denom



        def theta(t: float) -> float:
            f = ts.inst_forward(t)
            slope = df_dt(t)
            adj = (sig * sig) / (2.0 * a * a) * (1.0 - np.exp(-2.0 * a * t))
            return f + (1.0 / a) * slope + adj

        # store and return
        self.theta_fn = theta
        return theta

    # -------- Simulation of r_t with time-varying mean -----------------------

    def simulate_rates(self, N: int, grid: TimeGrid, r0: float, rng: np.random.Generator) -> np.ndarray:
        """
        Simulate r_t on the grid using the OU exact-variance step with time-varying mean Œ∏(t).
        For theta varying over the step, we use the standard 'midpoint' convolution:

            r_{k+1} = r_k * e^{-a dt} + (1 - e^{-a dt}) * theta(t_k + dt/2)
                      + sigma * sqrt((1 - e^{-2 a dt}) / (2 a)) * Z

        This is accurate and stable for small dt and smooth theta(t).

        Returns: array (N, K+1) with r at each grid time (including t0).
        """
        if self.theta_fn is None:
            # fallback: flat theta equal to r0 (keeps r stationary if sigma=0)
            self.theta_fn = lambda t: r0

        a, sig = self.kappa, self.sigma
        times = grid.times
        K = grid.K

        r = np.empty((N, K + 1), dtype=float)
        r[:, 0] = r0

        for k in range(K):
            dt = times[k + 1] - times[k]
            ead = np.exp(-a * dt)
            mean_conv = (1.0 - ead) * self.theta_fn(times[k] + 0.5 * dt)
            vol_step = sig * np.sqrt((1.0 - np.exp(-2.0 * a * dt)) / (2.0 * a))
            Z = rng.standard_normal(size=N)
            r[:, k + 1] = r[:, k] * ead + mean_conv + vol_step * Z

        return r


### FILE: src\rates\zc_pricer.py
"""
Zero-coupon pricer under HW1F++:

P(t,T) = A(t,T) * exp( -B(t,T) * r_t ),
with
  B(t,T) = (1 - e^{-kappa (T - t)}) / kappa,
  ln A(t,T) = - ‚à´_t^T theta(s) * (1 - e^{-kappa (T - s)}) ds  +  0.5 * Var(‚à´_t^T r_u du),

and for an OU short rate,
  Var(‚à´_t^T r_u du) = (sigma^2 / (2 kappa^3)) * ( 2 kappa Œî + 4 e^{-kappa Œî} - e^{-2 kappa Œî} - 3 ), Œî = T - t.

We compute the integral with a small adaptive trapezoidal rule (accurate and robust).
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Iterable, Callable

import numpy as np

from .hw1f import HW1FModel
from .termstructure.base_curve import TermStructure


@dataclass
class ZCAnalyticHW:
    hw: HW1FModel
    ts: TermStructure   # kept in case you want quick DF(0,T), not strictly required

    # ---- Core affine functions ----------------------------------------------

    def B(self, t: float, T: float) -> float:
        a = self.hw.kappa
        if T < t:
            raise ValueError("B(t,T): requires T >= t")
        Œî = T - t
        if a == 0:
            return Œî  # limit case
        return (1.0 - np.exp(-a * Œî)) / a

    def var_integrated_ou(self, delta: float) -> float:
        """Var( ‚à´_0^Œî r_u du ) for an OU short-rate's *noise* part; depends only on Œî."""
        a, sig = self.hw.kappa, self.hw.sigma
        if delta <= 0.0:
            return 0.0
        e1 = np.exp(-a * delta)
        e2 = np.exp(-2.0 * a * delta)
        return (sig * sig) / (2.0 * a**3) * (2.0 * a * delta + 4.0 * e1 - e2 - 3.0)

    def _integral_theta_weighted(self, t: float, T: float) -> float:
        """
        I(t,T) = ‚à´_t^T theta(s) * (1 - e^{-kappa (T - s)}) ds
        Composite trap√®ze avec pas adaptatif en fonction de Œî = T - t.
        """
        if self.hw.theta_fn is None:
            raise RuntimeError("ZCAnalyticHW: theta_fn not set on HW model.")
        if T <= t:
            return 0.0

        a = self.hw.kappa
        Œî = T - t
        # plus Œî est grand, plus on raffine :  ~80 sous-pas par ann√©e (ajuste si besoin)
        n = int(max(64, np.ceil(80.0 * Œî)))
        grid = np.linspace(t, T, n + 1)
        weights = 1.0 - np.exp(-a * (T - grid))
        theta_vals = np.array([self.hw.theta_fn(s) for s in grid], dtype=float)
        return float(np.trapz(theta_vals * weights, grid))


    def A(self, t: float, T: float) -> float:
        if T < t:
            raise ValueError("A(t,T): requires T >= t")
        Œî = T - t
        I = self._integral_theta_weighted(t, T)
        var_I = self.var_integrated_ou(Œî)
        lnA = - I + 0.5 * var_I
        return float(np.exp(lnA))

    # ---- Bond price ----------------------------------------------------------

    def P(self, t: float, r_t: float, T: float) -> float:
        """Zero-coupon price P(t,T) under HW1F++."""
        # SPECIAL CASE: exact recollage at t=0
        if abs(t) < 1e-14:
            return float(self.ts.discount_factor(T))
        return self.A(t, T) * np.exp(-self.B(t, T) * r_t)


    def P_vector(self, t: float, r_t: float, T_list: Iterable[float]) -> np.ndarray:
        T_arr = np.asarray(list(T_list), dtype=float)
        A_vals = np.array([self.A(t, T) for T in T_arr], dtype=float)
        B_vals = np.array([self.B(t, T) for T in T_arr], dtype=float)
        return A_vals * np.exp(-B_vals * r_t)


### FILE: src\rates\__init__.py


### FILE: src\rates\termstructure\base_curve.py
"""
Term structure base interface.

We work in *years*. Implementations must provide:
- zero_rate(T): y(0,T) in decimal (e.g., 0.025 for 2.5%).
- discount_factor(T): DF(0,T) = exp(-y(0,T)*T).
- inst_forward(t): instantaneous forward f(0,t).

Notes
-----
- All inputs are floats in years, T>=0.
- At T=0, we define DF(0)=1, and use right limits for zero/forward if needed.
"""

from __future__ import annotations
from abc import ABC, abstractmethod
import math


class TermStructure(ABC):
    @abstractmethod
    def zero_rate(self, T: float) -> float:
        """Zero-coupon continuously-compounded yield y(0,T)."""
        raise NotImplementedError

    @abstractmethod
    def discount_factor(self, T: float) -> float:
        """DF(0,T) = exp(-y(0,T)*T)."""
        raise NotImplementedError

    @abstractmethod
    def inst_forward(self, t: float) -> float:
        """Instantaneous forward rate f(0,t)."""
        raise NotImplementedError

    # ---- small helpers (optional default impls) ------------------------------

    def df_from_zero(self, T: float) -> float:
        """Default DF from zero rate (continuous compounding)."""
        if T < 0:
            raise ValueError("T must be >= 0")
        if T == 0.0:
            return 1.0
        y = self.zero_rate(T)
        return math.exp(-y * T)


### FILE: src\rates\termstructure\nelson_siegel.py
"""
Nelson‚ÄìSiegel term structure:
    y(0,T) = Œ≤0 + Œ≤1 * h(T) + Œ≤2 * ( h(T) - exp(-T/œÑ) ),
where h(T) = (1 - exp(-T/œÑ)) / (T/œÑ).

We provide:
- zero_rate(T)
- discount_factor(T) = exp(-y(0,T)*T)
- inst_forward(t) (instantaneous forward f(0,t))

We handle small T carefully (stable limits).
"""

from __future__ import annotations
import math
from dataclasses import dataclass

from .base_curve import TermStructure


@dataclass(frozen=True)
class NelsonSiegel(TermStructure):
    beta0: float
    beta1: float
    beta2: float
    tau: float

    def __post_init__(self) -> None:
        if self.tau <= 0:
            raise ValueError("NelsonSiegel: tau must be > 0")

    # ---- core building blocks ------------------------------------------------

    def _h(self, T: float) -> float:
        """
        h(T) = (1 - e^{-T/œÑ}) / (T/œÑ)
        Stable for small T: use series expansion h ~ 1 - (T/(2œÑ)) + (T/œÑ)^2/6 - ...
        """
        if T == 0.0:
            return 1.0
        x = T / self.tau
        if abs(x) < 1e-6:
            # 3-term series is enough here
            return 1.0 - 0.5*x + (x*x)/6.0
        return (1.0 - math.exp(-x)) / x

    def _h_prime(self, T: float) -> float:
        """
        h'(T) = d/dT [ (1 - e^{-T/œÑ}) / (T/œÑ) ]
              = [ (e^{-x} / œÑ)*x - (1 - e^{-x})*(1/œÑ) ] / x^2 , with x=T/œÑ
        We implement a numerically stable form and a small-T expansion.

        A simpler closed form:
          h'(T) = ( (T/œÑ)*e^{-T/œÑ} - (1 - e^{-T/œÑ}) ) / (T^2/œÑ)
                = ( œÑ / T^2 ) * ( (T/œÑ) e^{-T/œÑ} - (1 - e^{-T/œÑ}) )
        """
        if T == 0.0:
            # series: h(T) ‚âà 1 - x/2 + x^2/6 ; h'(T) = dh/dT = (dh/dx)*(dx/dT) with x=T/œÑ
            # dh/dx ‚âà -1/2 + (2x)/6 = -1/2 + x/3 ; at T=0 -> dh/dx ‚âà -1/2
            # so h'(0) = (-1/2)*(1/œÑ)
            return -0.5 / self.tau

        x = T / self.tau
        ex = math.exp(-x)
        numerator = (x * ex) - (1.0 - ex)     # (x e^{-x} - (1 - e^{-x}))
        return (self.tau / (T * T)) * numerator

    # ---- Nelson‚ÄìSiegel formulas ---------------------------------------------

    def zero_rate(self, T: float) -> float:
        if T < 0:
            raise ValueError("T must be >= 0")
        h = self._h(T)
        return self.beta0 + self.beta1 * h + self.beta2 * (h - math.exp(-T / self.tau))

    def discount_factor(self, T: float) -> float:
        if T < 0:
            raise ValueError("T must be >= 0")
        if T == 0.0:
            return 1.0
        y = self.zero_rate(T)
        return math.exp(-y * T)

    def inst_forward(self, t: float) -> float:
        """
        Instantaneous forward:
          f(0,t) = d/dt [ y(0,t) * t ] = y(0,t) + t * dy/dt.

        With y(0,t) = Œ≤0 + Œ≤1 h(t) + Œ≤2 (h(t) - e^{-t/œÑ}),
        dy/dt = Œ≤1 h'(t) + Œ≤2 ( h'(t) + (1/œÑ) e^{-t/œÑ} ).
        """
        if t < 0:
            raise ValueError("t must be >= 0")

        y = self.zero_rate(t)
        hp = self._h_prime(t)
        e = math.exp(-t / self.tau)
        dy_dt = self.beta1 * hp + self.beta2 * (hp + (1.0 / self.tau) * e)

        return y + t * dy_dt


### FILE: src\rates\termstructure\__init__.py


### FILE: src\sim\rng.py
"""
Random generators wrapper.

We centralize RNG creation so all modules share the same NumPy Generator
(ensures reproducibility across runs when a seed is provided).
"""

from __future__ import annotations
from dataclasses import dataclass
import numpy as np


@dataclass
class RNG:
    seed: int | None = None

    def __post_init__(self) -> None:
        self._gen = np.random.default_rng(self.seed)

    @property
    def gen(self) -> np.random.Generator:
        return self._gen

    # convenience methods (re-expose a few)
    def normal(self, size, mean=0.0, std=1.0) -> np.ndarray:
        return self._gen.normal(loc=mean, scale=std, size=size)

    def standard_normal(self, size) -> np.ndarray:
        return self._gen.standard_normal(size=size)

    def uniform(self, size) -> np.ndarray:
        return self._gen.uniform(size=size)


### FILE: src\sim\scenario_engine.py
"""
Portfolio simulator (no WWR, no dependence):
- Simulate HW1F++ rate paths once
- For the bank: simulate Log-OU credit, average PD/S over scenarios
- For each counterparty:
    * simulate Log-OU credit (independent), average PD/S
    * compute EPE/ENE from rate paths
    * compute CVA/DVA legs and aggregates

Returns structured dicts ready for export (step 10).
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Any

import numpy as np

from ..core.timegrid import TimeGrid
from ..rates.hw1f import HW1FModel
from ..rates.zc_pricer import ZCAnalyticHW
from ..rates.df_curve import DFCurveOnGrid
from ..products.swap import Swap
from ..credit.entities import Counterparty, Bank
from ..credit.log_ou_intensity import LogOUIntensity
from ..exposure.exposure_engine import ExposureEngine
from ..xva.cva_dva import CVAEngine


@dataclass
class Simulator:
    grid: TimeGrid
    rate_model: HW1FModel
    zc_pricer: ZCAnalyticHW
    bank: Bank
    rng: np.random.Generator
    df_curve_on_grid: DFCurveOnGrid

    # internal caches
    _rates_paths: np.ndarray | None = None          # (N, K+1)
    _bank_PD: np.ndarray | None = None              # (K+1,)
    _bank_S: np.ndarray | None = None               # (K+1,)

    # ---------- core helpers --------------------------------------------------

    def _ensure_rate_paths(self, N: int, r0: float) -> np.ndarray:
        if self._rates_paths is None or self._rates_paths.shape[0] != N:
            self._rates_paths = self.rate_model.simulate_rates(N, self.grid, r0, self.rng)
        return self._rates_paths

    def _simulate_bank_credit(self, N: int) -> tuple[np.ndarray, np.ndarray]:
        """
        Return (PD_bank_mean, S_bank_mean), each shape (K+1,).
        """
        if self._bank_PD is not None and self._bank_S is not None:
            return self._bank_PD, self._bank_S

        lou_b = self.bank.make_model()
        lam_b, S_b, PD_b = lou_b.simulate(N, self.grid, self.rng)
        self._bank_PD = PD_b.mean(axis=0)
        self._bank_S  = S_b.mean(axis=0)
        return self._bank_PD, self._bank_S

    # ---------- public API ----------------------------------------------------

    def run_for_counterparty(self, cpty: Counterparty, N: int) -> Dict[str, Any]:
        """
        Run the full pipeline for a single counterparty.

        Returns a dict with:
          - 'cid', 'LGD_cpty', 'LGD_bank'
          - 'DF' (K+1,), 'EPE', 'ENE', 'PD_cpty', 'S_cpty', 'PD_bank'
          - 'CVA_leg', 'DVA_leg', 'CVA', 'DVA'
        """
        Kp1 = self.grid.K + 1

        # 1) Rates
        r0 = self.zc_pricer.ts.inst_forward(0.0)  # coh√©rent HW++ √† t=0
        rates = self._ensure_rate_paths(N, r0)     # (N, K+1)

        # 2) Discount factors on grid
        DF = self.df_curve_on_grid.values()        # (K+1,)

        # 3) Credit
        lou_i = cpty.make_model()
        lam_i, S_i, PD_i = lou_i.simulate(N, self.grid, self.rng)
        PD_cpty = PD_i.mean(axis=0)                # (K+1,)
        S_cpty  = S_i.mean(axis=0)                 # (K+1,)

        PD_bank, S_bank = self._simulate_bank_credit(N)   # (K+1,), (K+1,)

        # 4) Exposure
        engine = ExposureEngine(self.zc_pricer)
        EPE, ENE = engine.epe_ene(rates, self.grid, cpty.swap)  # (K+1,)

        # 5) CVA / DVA
        cvares = CVAEngine.compute_all(
            DF=DF,
            LGD_cpty=cpty.LGD,
            LGD_bank=self.bank.LGD,
            EPE=EPE,
            ENE=ENE,
            PD_cpty=PD_cpty,     # marginale non conditionnelle
            PD_bank=PD_bank,     # marginale non conditionnelle
            S_cpty=S_cpty,       # survie cpty (le moteur d√©calera S_{k-1})
        )

        return {
            "cid": cpty.cid,
            "LGD_cpty": cpty.LGD,
            "LGD_bank": self.bank.LGD,
            "DF": DF, "EPE": EPE, "ENE": ENE,
            "PD_cpty": PD_cpty, "S_cpty": S_cpty, "PD_bank": PD_bank,
            "CVA_leg": cvares.cva_leg, "DVA_leg": cvares.dva_leg,
            "CVA": cvares.cva, "DVA": cvares.dva,
        }

    def run_portfolio(self, counterparties: List[Counterparty], N: int) -> Dict[str, Any]:
        """
        Run for all counterparties and produce aggregated totals.

        Returns
        -------
        dict with:
          - 'per_counterparty': list of dicts (see run_for_counterparty)
          - 'totals': {'CVA_total', 'DVA_total', 'CVA_legs_sum', 'DVA_legs_sum'}
          - 'meta': {'N', 'Kp1'}
        """
        if len(counterparties) == 0:
            raise ValueError("run_portfolio: empty counterparties list")

        # Force generation of rates + bank credit once
        r0 = self.zc_pricer.ts.inst_forward(0.0)
        self._ensure_rate_paths(N, r0)
        self._simulate_bank_credit(N)
        DF = self.df_curve_on_grid.values()
        Kp1 = DF.shape[0]

        per_cpty: List[Dict[str, Any]] = []
        cva_legs_sum = np.zeros(Kp1, dtype=float)
        dva_legs_sum = np.zeros(Kp1, dtype=float)
        CVA_total = 0.0
        DVA_total = 0.0

        for c in counterparties:
            res = self.run_for_counterparty(c, N)
            per_cpty.append(res)
            cva_legs_sum += res["CVA_leg"]
            dva_legs_sum += res["DVA_leg"]
            CVA_total += res["CVA"]
            DVA_total += res["DVA"]

        return {
            "per_counterparty": per_cpty,
            "totals": {
                "CVA_total": CVA_total,
                "DVA_total": DVA_total,
                "CVA_legs_sum": cva_legs_sum,
                "DVA_legs_sum": dva_legs_sum,
                "DF": DF,
            },
            "meta": {"N": N, "Kp1": Kp1, "n_counterparties": len(counterparties),
},
        }


### FILE: src\sim\__init__.py


### FILE: src\xva\cva_dva.py
"""
CVA / DVA engine (bucketed and aggregated).

Inputs (per counterparty i):
- DF[k]                : discount factors DF(0, t_k), shape (K+1,)
- LGD_cpty             : float in [0,1]
- LGD_bank             : float in [0,1]
- EPE[k], ENE[k]       : exposures from ExposureEngine, shape (K+1,)
- PD_cpty[k]           : *marginal non-conditional* PD on (t_{k-1}, t_k], PD[0]=0
- S_cpty[k]            : survival up to t_k, S[0]=1
- PD_bank[k]           : bank marginal non-conditional PD on (t_{k-1}, t_k], PD_bank[0]=0

Formulas (discrete buckets):
  CVA_leg[k] = DF[k] * LGD_cpty  * EPE[k] * PD_cpty[k]
  DVA_leg[k] = DF[k] * LGD_bank  * ENE[k] * S_cpty[k-1] * PD_bank[k]
with CVA_leg[0] = DVA_leg[0] = 0 by convention.

Notes:
- Arrays are *not* discounted exposures; DF enters only in the CVA/DVA legs.
- We accept any (K+1,) shape and enforce k=0 legs to 0.
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Any, Optional, Tuple

import numpy as np


def _as_1d(a, name: str) -> np.ndarray:
    a = np.asarray(a, dtype=float).ravel()
    if a.ndim != 1:
        raise ValueError(f"{name} must be 1D")
    return a


@dataclass
class CVAResults:
    cva_leg: np.ndarray   # (K+1,)
    dva_leg: np.ndarray   # (K+1,)
    cva: float
    dva: float


class CVAEngine:
    @staticmethod
    def cva_by_bucket(DF, LGD_cpty: float, EPE, PD_cpty) -> np.ndarray:
        DF = _as_1d(DF, "DF")
        EPE = _as_1d(EPE, "EPE")
        PD  = _as_1d(PD_cpty, "PD_cpty")
        if not (DF.shape == EPE.shape == PD.shape):
            raise ValueError("DF, EPE, PD_cpty must have the same shape (K+1,)")

        LGD_cpty = float(LGD_cpty)
        if not (0.0 <= LGD_cpty <= 1.0):
            raise ValueError("LGD_cpty must be in [0,1]")

        leg = DF * LGD_cpty * EPE * PD
        # convention: k=0 leg = 0 (PD[0] must be 0 but we enforce anyway)
        if leg.size > 0:
            leg[0] = 0.0
        return leg

    @staticmethod
    def dva_by_bucket(DF, LGD_bank: float, ENE, PD_bank, S_cpty, use_S_prev: bool = True) -> np.ndarray:
        DF = _as_1d(DF, "DF")
        ENE = _as_1d(ENE, "ENE")
        PD_b = _as_1d(PD_bank, "PD_bank")
        S    = _as_1d(S_cpty, "S_cpty")
        if not (DF.shape == ENE.shape == PD_b.shape == S.shape):
            raise ValueError("DF, ENE, PD_bank, S_cpty must have same shape (K+1,)")

        LGD_bank = float(LGD_bank)
        if not (0.0 <= LGD_bank <= 1.0):
            raise ValueError("LGD_bank must be in [0,1]")

        # S_prev[k] = S[k-1], with S_prev[0]=1 by convention
        if use_S_prev:
            S_prev = np.empty_like(S)
            S_prev[0] = 1.0
            if S.size > 1:
                S_prev[1:] = S[:-1]
        else:
            # allow caller to pass already-shifted survival in S
            S_prev = S

        leg = DF * LGD_bank * ENE * S_prev * PD_b
        if leg.size > 0:
            leg[0] = 0.0
        return leg

    @staticmethod
    def aggregate(leg: np.ndarray) -> float:
        leg = _as_1d(leg, "leg")
        # ensure non-negative up to tiny num noise
        leg = np.maximum(leg, 0.0)
        return float(np.sum(leg))

    @staticmethod
    def compute_all(
        DF, LGD_cpty: float, LGD_bank: float,
        EPE, ENE, PD_cpty, PD_bank, S_cpty
    ) -> CVAResults:
        cva_leg = CVAEngine.cva_by_bucket(DF, LGD_cpty, EPE, PD_cpty)
        dva_leg = CVAEngine.dva_by_bucket(DF, LGD_bank, ENE, PD_bank, S_cpty, use_S_prev=True)
        return CVAResults(
            cva_leg=cva_leg,
            dva_leg=dva_leg,
            cva=CVAEngine.aggregate(cva_leg),
            dva=CVAEngine.aggregate(dva_leg),
        )


### FILE: src\xva\shapley_explain.py
# src/xva/shapley_explain.py
from __future__ import annotations

import itertools
import math
from typing import Callable, Dict, List, Tuple

import numpy as np


def _as_1d(a, name: str) -> np.ndarray:
    a = np.asarray(a, dtype=float).ravel()
    if a.ndim != 1:
        raise ValueError(f"{name} must be 1D")
    return a


def _shift_survival(S_cpty: np.ndarray) -> np.ndarray:
    """S_prev[k]=S[k-1], with S_prev[0]=1."""
    S = _as_1d(S_cpty, "S_cpty")
    S_prev = np.empty_like(S)
    S_prev[0] = 1.0
    if S.size > 1:
        S_prev[1:] = S[:-1]
    return S_prev


def cva_leg(DF, LGD_cpty: float, EPE, PD_cpty) -> np.ndarray:
    DF = _as_1d(DF, "DF")
    EPE = _as_1d(EPE, "EPE")
    PD = _as_1d(PD_cpty, "PD_cpty")
    if not (DF.shape == EPE.shape == PD.shape):
        raise ValueError("DF, EPE, PD_cpty must have same shape")
    leg = DF * float(LGD_cpty) * EPE * PD
    if leg.size:
        leg[0] = 0.0
    return leg


def dva_leg(DF, LGD_bank: float, ENE, PD_bank, S_cpty) -> np.ndarray:
    DF = _as_1d(DF, "DF")
    ENE = _as_1d(ENE, "ENE")
    PDb = _as_1d(PD_bank, "PD_bank")
    S_prev = _shift_survival(S_cpty)
    if not (DF.shape == ENE.shape == PDb.shape == S_prev.shape):
        raise ValueError("DF, ENE, PD_bank, S_cpty must have same shape")
    leg = DF * float(LGD_bank) * ENE * S_prev * PDb
    if leg.size:
        leg[0] = 0.0
    return leg


def shapley_vector(
    base_inputs: Dict[str, np.ndarray],
    new_inputs: Dict[str, np.ndarray],
    feature_names: List[str],
    value_fn: Callable[[Dict[str, np.ndarray]], np.ndarray],
) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
    """
    Exact Shapley for vector-valued function using permutations.
    Returns:
      contribs: dict feature -> vector (same shape as value_fn output)
      delta: value(new) - value(base)
    """
    if set(feature_names) != set(base_inputs.keys()) or set(feature_names) != set(new_inputs.keys()):
        raise ValueError("feature_names must match keys of base_inputs and new_inputs")

    # Evaluate once to get shape
    v0 = _as_1d(value_fn(dict(base_inputs)), "v0")
    v1 = _as_1d(value_fn(dict(new_inputs)), "v1")
    if v0.shape != v1.shape:
        raise ValueError("value_fn(base) and value_fn(new) must have same shape")
    delta = v1 - v0

    contribs = {f: np.zeros_like(v0) for f in feature_names}

    perms = list(itertools.permutations(feature_names))
    for perm in perms:
        state = dict(base_inputs)
        v_prev = _as_1d(value_fn(state), "v_prev")
        for f in perm:
            state[f] = new_inputs[f]
            v_curr = _as_1d(value_fn(state), "v_curr")
            contribs[f] += (v_curr - v_prev)
            v_prev = v_curr

    m = float(math.factorial(len(feature_names)))
    for f in feature_names:
        contribs[f] /= m

    return contribs, delta


def shapley_cva_legs(
    DF0, EPE0, PD0,
    DF1, EPE1, PD1,
    LGD_cpty: float,
) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
    feats = ["DF", "EPE", "PD_cpty"]
    base = {"DF": _as_1d(DF0, "DF0"), "EPE": _as_1d(EPE0, "EPE0"), "PD_cpty": _as_1d(PD0, "PD0")}
    new  = {"DF": _as_1d(DF1, "DF1"), "EPE": _as_1d(EPE1, "EPE1"), "PD_cpty": _as_1d(PD1, "PD1")}

    def vf(inp: Dict[str, np.ndarray]) -> np.ndarray:
        return cva_leg(inp["DF"], LGD_cpty, inp["EPE"], inp["PD_cpty"])

    return shapley_vector(base, new, feats, vf)


def shapley_dva_legs(
    DF0, ENE0, PD_bank0, S_cpty0,
    DF1, ENE1, PD_bank1, S_cpty1,
    LGD_bank: float,
) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
    feats = ["DF", "ENE", "PD_bank", "S_cpty"]
    base = {
        "DF": _as_1d(DF0, "DF0"),
        "ENE": _as_1d(ENE0, "ENE0"),
        "PD_bank": _as_1d(PD_bank0, "PD_bank0"),
        "S_cpty": _as_1d(S_cpty0, "S_cpty0"),
    }
    new = {
        "DF": _as_1d(DF1, "DF1"),
        "ENE": _as_1d(ENE1, "ENE1"),
        "PD_bank": _as_1d(PD_bank1, "PD_bank1"),
        "S_cpty": _as_1d(S_cpty1, "S_cpty1"),
    }

    def vf(inp: Dict[str, np.ndarray]) -> np.ndarray:
        return dva_leg(inp["DF"], LGD_bank, inp["ENE"], inp["PD_bank"], inp["S_cpty"])

    return shapley_vector(base, new, feats, vf)


### FILE: src\xva\__init__.py


